\chapter{Deep Learning Theory}

Although the field of deep learning has been growing at an incredibly fast rate, there is still a lot of notions in deep learning that remains not understood. Because it is rather simple to implement and it doesn't require a lot of ingenuity to design a new architecture, a lot of empirical advancements have been published. However, because of the nature of the field, and a bad selection of reviewers for publications/conferences*, there isn't a lot of fundamental understanding of what is happening with deep learning. This becomes extremely crucial for healthcare, as the nature of that field requires a complete understanding of the reasoning behind diagnostic in order to diagnose a patient. Although DL is very powerful and it is able to approach functions with a very high degree of complexity, they are unable to enter the field of healthcare due to the lack of understanding. In other words, how would a patient respond if the hospital provided a diagnose of a deadly disease/disorder, and when asked why, the response would be: "Because the deep neural network with $X$ accuracy said so." In this chapter, four different approaches are described in each section attempting to provide a better understanding of deep learning.

* \textit{It occurred last semester that one of our collegues (a PhD student) in Georgia Tech was invited to review papers for NIPS because he had a few publications in deep learning. He was in fact stressed about it and asking for help as he had never reviewed any publications before, and he was uncertain how to do so.}

\textbf{Note that each section has their own independent notation.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Stochastic Thermodynamics of Learning \cite{thermo}}
The first field that is described follows the field of thermodynamics and a little bit of information theory. Note that, although showing the work from \cite{thermo}, I am changing some of the notations to a set of notations that is a bit more familiar.

First, modeling a neuron, let a neuron make $N$ connections to ther neurons, which is fully characterized by $\bm{w} \in \mathbb{R}^N$, and it must learn whether or not to fire an action potential from a set of $P$ different examples $\bm{x}^p = \{ (x_1^p,x_2^p,\cdots,x_N^p) | 1\leq p \leq P \}$ (i.e. patterns), which describe the activity of all other connected neurons, and say that $x^p_n \in \{ -1, 1\}$, which represents the $n^{th}$ feature (i.e. the active/inactive state coming from the $n^{th}$ neuron) of the $p^{th}$ observation. The observations $\bm{x}^p$ are paired with a ground truth label $y^p \in \{-1,1\}$, and the neuron tries to predict a label $\hat{y}^p \in \{-1,1\}$ such that it equates the true label $y^p$. The neuron may be modeled as a thermal environment with the transition rates as $k_p^\pm$ for the equilibrium between the states -1 and +1. The equilibrium process can be modeled as the following, where $\mathcal{A}^p$ is the input-dependent function, $k_B$ is the Boltzmann's constant, and $T_{temp}$ represents a fixed temperature.

\begin{align*}
  \frac{k_p^+}{k_p^-} &= \exp\left(\frac{\mathcal{A}^p}{k_B T_{temp}} \right) \\
  \mathcal{A}^p &= \frac{\bm{w} \cdot \bm{x}^p}{\sqrt{N}}
\end{align*}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.6\textwidth]{"./images/neuron_model".png}
    \caption{The model of a neuron, where the $p^{th}$ input-target pair is used to compute $\mathcal{A}^p$, which then is used to compute the transition rates ratio between the two labels $\{-1,+1\}$}
    \label{fig::model_neuron}
\end{center}
\end{figure}

This work presents the concept of \textit{learning efficiency}, but first, a few concepts need to be established. In such case, for simplicity, set $k_B = T = 1$ in order to render the energy and entropy dimensionless. Considering a network with one weight, one sample, and one label ($N=P=1$), the weight $w$ is assumed to follow the overdamped Langevin equation

$$ \frac{dw(t)}{dt} = - w(t) + f(w(t) , \bm{x}, y, t) + \zeta(t) $$

\begin{itemize}
    \item $f(\cdot) =$ external force, which depents on the learning algorithm chosen. It creates the correlation between the weight $w$ and the input $\bm{x}$
    \item $\zeta (t) = $ Gaussian thermal noise, such that the correlation $\langle \zeta(t)\zeta(t') \rangle = 2 \delta(t-t')$
\end{itemize}

By setting the initial conditions occurring at $t=t_0 = 0$, the weight is in thermal equilibrium (i.e. $Pr(w) \propto e^{-\frac{w^2}{2}}$), and the labels are equiprobable (i.e. $Pr(y) = Pr(\hat{y}) = 0.5)$. With symmetric rates,
\begin{align*}
    k_\pm &= \gamma e^{frac{A}{2}} \\
    \frac{k_+}{k_-} = \frac{\gamma \exp{\frac{A}{2k_B T_{temp}}}  }{\gamma \exp{-\frac{A}{2k_B T_{temp}}} } = \exp{\frac{A}{2k_B T_{temp}}} \gamma \exp{\frac{A}{2k_B T_{temp}}} = \gamma \exp{\frac{A}{k_B T_{temp}}}
\end{align*}

Therefore, the partial differential equation for the joint probability distribution $Pr(y,w,\hat{y},t)$ is defined as
\begin{equation*}
\begin{dcases}
    \frac{\partial Pr(y,w,\hat{y},t)}{\partial t} = - \frac{\partial j_w (t)}{\partial w} + j_{\hat{y}}(t) \\
    j_w(t) = \left[ -w + f(w,\bm{x},y,t) - \frac{\partial}{\partial w}\right] Pr(y,w,\hat{y}, t) & = \text{Probability current for weight}\\
    j_{\hat{y}}(t) = k_{\hat{y}} Pr(y,w,\hat{y},t) - k_{-\hat{y}} Pr(y,w,\hat{y},t) & = \text{Probability current for the predicted label}
\end{dcases}
\end{equation*}

Per definition, a probability current (i.e. probability flux) is the mathematical quantity describing the flow of probability in terms of probability per unit time. It is quite similar to if one was to describe the probability density as a fluid, and the probability current is the rate of flow of the fluid.

\subsection{Learning Efficiency}
To measure the learning efficiency, one must first define the Shannon entropy $S(X)$ of a random variable $X$, the conditional entropy of $X$ given $Y$, $S(X|Y)$ and the mutual information $I(y:\hat{y})$

\begin{equation*}
\begin{dcases}
    S(Y) = -\sum_{y \in Y} Pr(y) \log(Pr(y)) \\
    S(Y|\hat{Y}) = - \sum_{y,\hat{y}} Pr(y,\hat{y}) \log(Pr(y|\hat{y})) = - \sum_{y,\hat{y}} Pr(y,\hat{y}) \log \left( \frac{Pr(y,\hat{y})}{Pr(\hat{y})}\right) \\
    I(y:\hat{y}) = S(y) - S(y|\hat{y})
\end{dcases}
\end{equation*}

The $S(Y)$ measures the uncertainty of $Y$, and the mutual information is a natural quantity which measures the information learned. In other words, it measures the different between the uncertainty of the true label $y$ and the uncertainty of the true label $y$ given $\hat{y}$. It is then possible to measure the mutual information $I(y:\hat{y})$ to the thermodynamic costs of adjusting the weight from time $t_0$ to time $t$, with the total entropy production.

\begin{equation*}
    \begin{dcases}
        \Delta S_w^{tot} = \Delta S(w) - \Delta Q  \\
        \Delta S(w) = \text{difference of Shannon entropy of } P(w,t) = \sum_{y,\hat{y}} Pr(y,w,\hat{y},t)\text{ from time $t_0$ to $t$} \\
        \Delta Q = \text{heat dissipated into the medium by the dynamics of the weight}
    \end{dcases}
\end{equation*}

Their work show that the thermodynamic costs of learning in fact bound the information learned with
$$I(y:\hat{y}) \leq \Delta S(w) + \Delta Q$$

for any arbitrary algorithm $f(w,\bm{x},y,t)$ at all times $t > t_0$. This is important because they relate the entropy production in the weights with the change in mutual information between $y$ and $\hat{y}$. Therefore, one may define \textit{learning efficiency} $\eta$ as

$$ \eta = \frac{I(y,\hat{y})}{\Delta S(w) + \Delta Q} \leq 1$$

In a toy problem, they formulate the learning efficiency of a Hebbian learning, where $N=P=1$ as $t\rightarrow \infty$. This makes $\bm{x} = x$ as a single input. The Hebbian learning formulation can be described as

$$w_{ij}(t+1) = w_{ij}(t) + \nu x_i(t) x_j(t)$$

where $\nu$ is the learning rate coefficient, and $x$ are the outputs of the $i^{th}$ and $j^{th}$ coefficient. Thus, for a neuron, when the input neuron fires, then the following neuron should fire,and the weight of their connection increases. If the following neuron has a response that differs from the input neuron, then the connection decreases. This means that $x = y = \pm 1$. Therefore, this yields a final weight that is proportional to the $\mathcal{F}$ which is defined as $\mathcal{F} = y x$. Then they choose a learning force $f$ that linearly increases with time

\begin{equation*}
f(w,x,y,t) = \begin{dcases}
\frac{\nu \mathcal{F}}{\tau} = \frac{\nu y x}{\tau} & t \leq \tau \\
\nu \mathcal{F} = \nu y x  & t \leq \tau
\end{dcases}
\end{equation*}

where $\tau > 0$ represents the learning duration and $\nu > 0$ represents the learning rate in machine learning. One can compute the total entropy loss $\Delta S_w ^{tot}$, by computing $Pr(y,w,t)$ and $\Delta Q$. To compute $Pr(y,w,t)$, one should
\begin{enumerate}
    \item Integrate $\hat{y}$ out of $\frac{\partial}{\partial t} Pr(y,w,\hat{y},t) = -\frac{\partial}{\partial w} j_w(t) + j_{\hat{y}}(t)$, which yields a Fokker-Planck equation
    \item Solve the Fokker-Planck equation to obtain $Pr(y,w,t)$
\end{enumerate}

In order to calculate $\Delta Q$, where $f = f(w(t), x, y, t)$, it is computed as

$$\Delta Q = \int_0^\infty dt \int_{-\infty}^\infty dw j_w(t) [-w(t) + f] = \frac{\nu^2 \mathcal{F} (e^{-\tau}+\tau - 1)}{\tau^2}$$

IF the learning duration is too long, no heat is dissipated, but if the learning duration is too sudden, then it the following solutions are
\begin{equation*}
\begin{dcases}
    \lim_{\tau \rightarrow \infty} \Delta Q = 0 \\
    \lim_{\tau \rightarrow 0} \Delta Q = \frac{\nu^2 \mathcal{F}^2}{2}
\end{dcases}
\end{equation*}

Then the Shannon entropy $\Delta S(w)$ can be computed from the marginal probability $Pr(w,t) = \sum_y Pr(y,w,t)$ which was obtained while computing $\Delta S_w^{tot}$. The stationary solution of a differential equation is defined as when $\frac{\partial \hat{f}}{\partial t}$ is zero, and the solution to the stationary solution of

$$\frac{\partial Pr(y,w,\hat{y},t)}{\partial t} = - \frac{\partial j_w (t)}{\partial w} + j_{\hat{y}}(t) = 0$$

yields the mutual information $I(y:\hat{y})$. This then is enough to create the plot shown in Figure \ref{fig::learning_efficiency}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.4\textwidth]{"./images/learning_efficiency_graph".png}
    \caption{The learning efficiency profile with varying learning duration $\tau$ and varying learning rate $\nu$, with a Hebbian learning problem}
    \label{fig::learning_efficiency}
\end{center}
\end{figure}

This however is not a very feasible profile for any learning algorithm. In other words, to set the learning rate to as high as possible may not always work in factor to the optimization problem.

By extending this problem to a multidimentional problem, some of the formulation changes a little bit. A typical neuron usually connects to 1000 other neurons ($N\neq 1$), and it has multiple samples (i.e. $P\neq 1$). Therefore, with $\hat{\bm{y}} \in \mathbb{R}^{P}$, having the initial conditions at $t_0$ yield equiprobable results (i.e. $Pr(y) = Pr(\hat{y}) = 0.5$), the quantity that describes the amount of learning after a time $t$ has to be the sum of mutual information $I(y:\hat{y})$ over all inputs. They show that the information is bounded by the total entropy production for all of the weights

$$\sum_{p=1}^P I(y^p,\hat{y}^p) \leq \sum_{n=1}^N \Delta S(w_n) + \Delta Q_n $$

They set the problem by letting $P$ samples and $N$ dimensions go to infinity while keeping a ratio $\alpha = P/N$ on the order of 1, by setting a learning force the same as before, but substituting $\mathcal{F}$ with $\mathcal{F}_n$, such that

$$\mathcal{F}_n = \frac{1}{\sqrt{N}} \sum_{p=1}^P x^p_n y^p $$

By averaging over the noise with fixed $\bm{y}$, $w_n$ is normally distributed with mean $\langle w_n \rangle = \nu \mathcal{F}_n$, and variance 1. By averaging with respect to the quenched disorder $\bm{y}$, $\mathcal{F}_n$ is normally distrubed with $\bar{\mathcal{F}_n} = 0$, $\bar{\mathcal{F}_n^2} = \alpha$, yielding $\bar{\langle w_n \rangle} = 0$ and $\bar{\langle w_n^2 \rangle} = \log(1+ \alpha \nu^2)$. Therefore,

$$\Delta S (w) = \log(1+\alpha \nu ^2)$$

From here, the heat dissipated by the $n^{th}$ weight $\bar{\Delta Q_n}$ is obtained by averaging
$$\Delta Q = \frac{\nu^2 \mathcal{F}^2 ( e^{-\tau} + \tau - 1 )}{\tau ^2}$$
 over $\mathcal{F} \rightarrow \mathcal{F}_n$. The mutual information $I(y^p:\hat{y}^p)$ is a functional of the marginalized distribution $Pr(y^p,\hat{y}^p)$, which can be found with

\begin{equation*}
\begin{dcases}
I(y^p:\hat{y}^p) = \log(2) - S[Pr(y^p = \hat{y}^p)] \\
S[p] = -p \log(p) - (1-p) \log(1-p)
Pr(y^p = \hat{y}^p) = \int_{-\infty}^\infty d\Delta^p Pr(\Delta^p) \frac{e^{\Delta^p}}{e^{\Delta^p} + 1} \\
\Delta^p = \frac{1}{\sqrt{N}} \bm{w} \cdot \bm{x}^p y^p = \bm{\mathcal{A}}^p y^p
\end{dcases}
\end{equation*}

This can be used to compute the Hebbian learning efficiency $\tilde{\eta}$ as

$$\tilde{\eta} = \alpha \frac{I(y^p:\hat{y}^p)}{\Delta S(w_n) + \bar{\Delta Q_n}}$$

Hence, if a Monte-Carlo integration of $Pr(\bm{y},\bm{w},\hat{\bm{y}})$ with $N=10,000$, one obtains the following graphs.
\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.6\textwidth]{"./images/learning_efficiency_graph2".png}
    \caption{The mutual information profile with varying learning rates $\nu$ on with a Hebbian learning problem. The inset graph shows the learning efficiency $\tilde{\eta}$ in the limits $\tau \rightarrow 0$ (solid) and $\tau \rightarrow \infty$ (dashed)}
    \label{fig::learning_efficiency2}
\end{center}
\end{figure}

It is important to note that as $P$ increases, the mutual information decreases because of hte thermal noise in the system, and the well-known failure of Hebbian learning to use information in the samples perfectly. In both plots, $\nu$ increases from bottom to top.

This paper was very interesting because, although it was a full learning efficiency analysis over a very specific type of learning, Hebbian learning, which is connected to a fully connected layer architecture in deep learning, it does open the idea to try to expand the same formulations to different paradigms in deep learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deep Relaxation: PDEs for Optimizing Deep Neural Networks \cite{deeprelaxation}}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth]{"./images/deeprelax_1".png}
    \caption{A 1D loss profile of different formulations of the loss function, and the location of which algorithms converge to as shown with their respective colors but with a transparency value lower than 1 }
    \label{fig::deeprelax_1}
\end{center}
\end{figure}

This paper was another example that was the analysis over a very specific part of deep learning. In a previous work, \cite{entropySGD} had introduced a new type of stochastic gradient descent algorithm, the Entropy-SGD algorithm, which was shown to perform better than optimizers such as the Adam optimizer, yielding a modified loss function, called "local entropy" $f_\gamma(x)$

\begin{align*}
    f_\gamma(x) &= -\log \left( G_\gamma * e^{-f(x)}\right) \tag{local entropy} \\
    G_\gamma (x) &= (2\pi \gamma)^{-N/2} \exp\left( -\frac{|x|^2}{2\gamma}\right) = \text{Gaussian heat kernel}  \\
    f(x) &= \text{a loss function (e.g. cross-entropy loss w/ or w/o a regularizer)} \\
    x &= \text{weights of the neural network where } x \in \mathbb{R}^N
\end{align*}

where $\gamma$ represents the variance (or covariance) in the Gaussian kernel.

\subsection{PDE Interpretation of local entropy}

It is well known that the solution to the heat equation $u_t = \frac{1}{2} \Delta u$ with an initial condition $u(x,0) = f(x)$ yields the solution
$$u(x,t) = G_t * f(x)$$

As we know, although the $\gamma$ in (local entropy) represents variance, through the heat equation, it can easily be depicted as a time variable $t$. Then one can prove that the (local entropy) $f_\gamma(x)$ is the solution $u(x,t)$ of the viscous Hamilton-Jacobi PDE

\begin{equation*}
    \frac{\partial u}{\partial t} = -\frac{1}{2} | \nabla u |^2 + \frac{1}{2} \Delta u \quad \text{ where } 0 < t <\gamma \tag{viscous HJ}
\end{equation*}
 \begin{tcolorbox}[breakable]
\begin{proof}

Define a generalized solution
$$u(x,t) = - \log (\nu(x,t))$$
Then the equation $\nu(x,t) = e^{-u(x,t)}$ solves the heat equation $\nu_t = \frac{1}{2} \Delta \nu$, given $\nu(x,0) = e^{-f(x)}$. Therefore, the partial derivatives are defined as
\begin{equation*}
\begin{cases}
\nu_t = -\nu u_t  \\
\nabla \nu = -\nu \nabla u \\
\Delta \nu = - \nu \Delta u + \nu | \nabla u | ^2
\end{cases}
\end{equation*}

This then yields
\begin{align*}
    \nu_t &= \frac{1}{2} \Delta \nu \\
    -\nu u_t &= \frac{1}{2}\left( - \nu \Delta u + \nu | \nabla u | ^2 \right) \\
    u_t &= \frac{1}{2}\left( \Delta u - | \nabla u | ^2 \right) = - \frac{1}{2}  | \nabla u | ^2 + \frac{1}{2} \Delta u\\
\end{align*}
\end{proof}
\end{tcolorbox}

Then they show that local entropy is a more powerful way to smooth than with gradient averaging, as well as to derive a simple update rule for the Entropy-SGD algorithm. But first, one needs to show that the gradient of the local entropy $\nabla f_\gamma(x)$ can be shown in two ways:

\begin{align*}
\nabla f_\gamma(x) = \nabla u(x,\gamma) =&
\begin{dcases}
\int_{\mathbb{R}^n} \frac{x-y}{\gamma} \rho_1^\infty(dy;x) \\
\int_{\mathbb{R}^n} \nabla f(x-y) \rho_2^\infty(dy;x)
\end{dcases} \\
\text{ where }&
\begin{dcases}
\rho_1^\infty(y;x ) = Z_1^{-1} \exp\left( -f(y) - \frac{1}{2t} |x-y|^2 \right) \\
\rho_2^\infty(y;x ) = Z_2^{-1} \exp\left( -f(x-y) - \frac{1}{2t} |y|^2 \right) \\
\end{dcases}
\end{align*}

This can then be extended towards the non-viscous form of the Hamilton-Jacobi equation

\begin{equation*}
    \frac{\partial u}{\partial t} = - \frac{1}{2} | \nabla u |^2 \tag{non-viscous HJ}
\end{equation*}

where it is just like the (viscous HJ), but the coefficient in front of the Laplacian operator $\Delta$ is zero, which makes a simpler formula for the gradient. One may define, as shown in \textbf{Lemma 5} in \cite{deeprelaxation}, that if $u(x,t)$ is the viscosity solution of (HJ) with $u(x,0) = f(x)$,

\begin{equation}
    u(x,t) = \inf_y \left\{ f(y) + \frac{1}{2t} |x-y|^2 \right\} \tag{HL}
\end{equation}
Define the proximal operator (defined in the community) as

\begin{equation*}
    y^* = prox_{tf}(x) = \arg \min_y \left\{ f(y) + \frac{1}{2t} |x-y| ^2 \right\}
\end{equation*}

If the proximal operator is a singleton, $\nabla_x u(x,t)$ exists, which is

\begin{equation} \label{eq::gradx_u}
        \nabla_x u(x,t) = \frac{x-y^*}{t} = \nabla f(y^*)
\end{equation}

Using this, the discrete-time gradient descent dynamics using \ref{eq::gradx_u} is

$$x_{k+1} = x_k - \eta t^{-1} (x_k - prox_{tf}(x_k))$$

where $\eta > 0$ as the step-size. If $\eta = t$, then it becomes the proximal point iteration

$$x_{k+1} = prox_{tf}(x_k)$$

The standard proximal gradiant descent is given by

$$x_{k+1} = prox_{th}(x_k - t \nabla g(x_k))$$

where the loss function is made up of a convex function $g(x)$, and a potentially non-differentiable regularizer $h(x)$. The proximal operator equals to the implicit gradient descent, which is led due to the non-viscous HJ, shown as \ref{implicit_graddesc1}. Note how it differs from the gradient descent update, shown in \ref{graddesc}

\begin{equation}
    x_{k+1} = x_k - \eta \nabla f(x_{k+1}) \label{implicit_graddesc1}
\end{equation}
\begin{equation}
    x_{k+1} = x_k - \eta \nabla f(x_k)  \label{graddesc}
\end{equation}
\subsection{Derivation via Homogenization}
Homogenization is a technique highly used in dynamical systems that have multiple time scalesm, and it couples a few fast variables with other variables that evolve very slowly. Therefore, in the limit that the time scales separate, this allows one to compute averages over the very fast variables, which then allows us to obtain averaged equations for the slow variables. Elastic-SGD is an algorithm by \cite{elasticSGD} in order to minimize communication overhead between a set of workers that optimize replicated copies of the original function in distributed training of neural networks. Homogenization is then used to show that Elastic-SGDs are in fact minimzing the local entropy under ergodic conditions. Consider the following stochastic differential equations (SDEs)

\begin{align}
    dx(s) &= h(x,y) ds \nonumber \\
    dy(s) &= \frac{1}{\epsilon} g(x,y) ds + \frac{1}{\sqrt{\epsilon}} dW(s) \label{eq::SDEeqns}
\end{align}

where $h,g$ are sufficiently smooth functions, $W(s)$ represents the Wiener process, and the parameter $\epsilon > 0 $ is the homogenization parameter. Therefore, in the limit of $\epsilon \rightarrow 0$, if the unique invariant measure $\rho^\infty(y;x)$ exists and it is ergodit, then the dynamics of $x(s)$ converge to
\begin{equation*}
\begin{dcases}
    dX(s) = \bar{h}(X) ds \\
    \bar{h}(X) = \int h(X,y) \rho^\infty (dy; X)
\end{dcases}
\end{equation*}

where $\bar{h}(X)$ represents the homogenized vector field for $X$. The gradient of the local entropy has already been defined as

$$\nabla f_\gamma (x) = \gamma^{-1} \int_{\mathbb{R}^N} (x-y) \rho_1^\infty (dy;x)$$
Therefore we can say that
\begin{align*}
    h(x,y) &= -\gamma^{-1} (x-y) \\
    g(x,y) &= \nabla f(y) + \frac{1}{\gamma} (y-x)
\end{align*}

Then \ref{eq::SDEeqns} becomes
\begin{align}
    dx(s) &=  -\gamma^{-1} (x-y) ds \nonumber \nonumber \\
    dy(s) &= -\frac{1}{\epsilon} \left[ \nabla f(y) + \frac{1}{\gamma} (y-x)\right] ds + \frac{1}{\sqrt{\epsilon}} dW(s) \label{EntropySGD}
\end{align}

While a standard neural network optimizes the optimization problem

$$ x^* = \arg \min_x f(x)$$

an Elastic-SGD solves
$$\arg \min_{x^1,\cdot,x^n } \sum_{i=1}^n f(x^i) + \frac{1}{2\gamma} |x^i - x | 2 $$

Therefore, this yields
\begin{align}
    dx(s) &=  -\gamma^{-1} \sum_{i=1}^n (x-x^i) ds \nonumber \\
    dx^i(s) &= - \frac{1}{\epsilon} \left[ \nabla f(x^i) + \frac{1}{\gamma} ((x^i-x)\right] ds + \frac{1}{\sqrt{\epsilon}} dW^i (s)
\end{align}

The homogenized vector field can the be given by

\begin{align*}
    \bar{h}(X) &= \gamma^{-1} \int (x^1 - X) \rho^\infty(dx^1; X)
\end{align*}

Where all of these formulations match exactly the homogenized dynamics of Entropy-SGD. When comparing the heat equation to the viscous Hamilton-Jacobi equation, one may choose to study the equation below, as the convolution of hte exponentiated loss is quite complicated.

$$f_\gamma(x) = G_\gamma * f(x) $$

Since $\nabla f_\gamma ^2$ represents the averaging of the original gradient $\nabla f(x)$ over Gaussian pertubations of variance $\gamma$, the gradient descent dynamics of $f_\gamma^2$ may be written as
\begin{align*}
    dx(s) &= - \nabla f(x-y) ds \\
    dy(s) &= - \frac{1}{\epsilon \gamma} y ds + \frac{1}{\sqrt{\epsilon}} dW(s)
\end{align*}

It is important to note that for the heat equation, $y(s)$ has no dependencies on $x(s)$, whereas Entropy-SGD in does have it, on the term $\nabla f(x-y)$. Although the heat equation smoothing is a lot more performed, the Entropy-SGD does have a much better empirical performance.

\subsection{Stochastic Optimal Control Interpretation}
The fact that the local entropy $f_\gamma(x)$ can be formulated as a viscous Hamilton-Jacopy eqation, and it is now possible to the gradient descent as an optimal control problem. Therefore, consider the following controlled SDE

\begin{equation}
dx(s) = - \nabla f(x) ds - \alpha(s) ds + dW(s) \tag{CSGD}
\end{equation}
where $t \leq s \leq T$, $x=x(t)$, and $\alpha(\cdot)$ is the control. With a terminal cost function $V(x(T)) : \mathbb{R}^N \rightarrow \mathbb{R}$, one can represent the prototypical quadratic running cost

$$C(x,\alpha) = \mathbb{E} \left[ V(x(T)) + \frac{1}{2} \int_0^T |\alpha(s)|^2 ds \right]$$

If one can then set the minimum expected cost over all controls and paths to be
$$ u(x,t) = \min_{\alpha(\cdot)} C(x(\cdot),\alpha(\cdot))$$

Then one can set $u(x,t)$ to be the unique viscosity solution of the Hamilton-Jacobi-Bellman PDE

$$-\frac{\partial u}{\partial t} = -\nabla f(x) \cdot \nabla u(x,t) - \frac{1}{2} |\nabla u|^2 + \frac{1}{2} \Delta u$$

with a terminal condition $u(x,T) = V(x)$. This specific case leads the optimal control to be the gradient of the solution $$\alpha^*(x,t) = \nabla u(x,t)$$

Then, they finally show that, assuming $x_{csgd}(s)$ be the solution for the controlled stochastic gradient descent (CSGD) and $x_{sgd}(s)$ be the solution for the stochastic gradient descent (SGD), where they are shown below again for the convenience of the reader
\begin{equation}
    dx(t) = - \nabla f(x) dt - \alpha(t) dt + dW(t) \tag{CSGD}
\end{equation}
% \begin{equation}
%     dx(t) = - \nabla f(x) dt - + dW(t) \tag(SGD)
% \end{equation}

They show that
\begin{equation*}
    \mathbb{E}[V(x_{csgd}(t))] \leq \mathbb{E}[V(x_{sgd}(t))] - \frac{1}{2} \mathbb{E} \left[ \int^t _0 |\alpha^*(x_{csgd},s)|^2 ds \right]
\end{equation*}

% \subsection{The local entropy is the solution of the viscous Hamilton-Jacobi PDE}
% \subsection{The Elastic-SGD is equivalent to the local entropy under certain conditions}
% \subsection{Applications for Non-convex Optimization}
As results, the authors compare the four different algorithms to test their effectiveness. For each of them, $L$ represents the number of gradient evaluations performed prior to a weight update, and thus the number of epochs times $L$ is a measure of the wall-clock time, since a larger $L$ leads to a slower computation time.
\begin{itemize}
    \item Entropy-SGD ($L=20$)
    \item Smoothing by the HEAT equation ($L=5$)
    \item Non-viscous Hamilton-Jacobian Equation ($L=5$)
    \item Stochastic Gradient Descent ($L=1$)
\end{itemize}

All of these experiments are then performed under three different networks with the used datasets in parenthesis
\begin{itemize}
    \item mnistfc (MNIST)
    \item LeNet (MNIST)
    \item All-CNN (CIFAR-10)
\end{itemize}

One very interesting thing about this paper was the introduction of understanding how to choose certain hyperparameters; most specifically $\gamma$. As we have seen before, $\gamma$ is a parameter for the Gaussian kernel, which is also used to represent time, and the higher the value, the better it is to smooth a function. By scoping through the loss function, they provide a decay rule so that it allows for a smooth loss function in the beginning to quickly progress down, and to then preserve the locations of the local minima in the end as $\gamma \rightarrow 0$, where $k/L$ is an integer, and $\gamma_0 \in [10^4, 10]$

$$\gamma = \gamma_0 (1-10^{-3})^{k/L}$$

Below are a table of the results, where it summarizes the minimum validation error (\%) @ effective epochs.

\begin{center}
    \begin{tabular}{c c c c c}
        \hline
        & \multicolumn{4}{c}{Item} \\
        \cline{2-5}
        Model & Entropy-SGD & HEAT & HJ & SGD \\
        \hline
        mnistfc & \textbf{1.08 $\pm$ 0.02 \% @ 120} & 1.13 $\pm$ 0.02 \%@ 200 & 1.17 $\pm$ 0.04 \%@ 200 & 1.10 $\pm$ 0.01\% @ 149 \\
        LeNet & 0.5 $\pm$ 0.01\% @ 80 & 0.59 $\pm$ 0.02 \%@ 75 & \textbf{0.5 $\pm$ 0.01\% @ 70 } & 0.5 $\pm$ 0.02\% @ 67 \\
        All-CNN & 7.96 $\pm$ 0.05\% @ 160 & 9.04 $\pm$ 0.04\% @ 150 & \textbf{7.89 $\pm$ 0.07\% @ 145 } & 7.94 $\pm$ 0.06\% @ 195
    \end{tabular}
\end{center}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.3\textwidth]{"./images/deeprelax_mnistfc".png}
    \includegraphics[width=0.3\textwidth]{"./images/deeprelax_lenet".png}
    \includegraphics[width=0.3\textwidth]{"./images/deeprelax_allcnn_val".png}
    \caption{The figure above portrays the different progressions of the four previously mentioned methods on the (left) mnistfc network, (center) the LeNet, and (right) All-CNN network, showing that the HJ methodology often times achieves lower losses at a faster rate than the currently used optimization methods in the community}
    \label{fig::model_neuron}
\end{center}
\end{figure}

As the table shows, the Hamilton-Jacobi implementation will often lead to a lower expected percent error at a lower epoch. This means that by solving a stochastic control problem to the viscous HJ PDE yields a smaller expected loss as compared to the stochastic gradient descent. This work also provides a better understanding to the choice of parameters. More specifically, $\gamma$ is equivalent to time for the gradient flow in the local entropy and Elastic-SGD.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Characterization of Neural Networks as an Encoder-Decoder with Mutual Information \cite{blackbox}}
This work was another very interesting work, in which they approached the analysis of deep learning from the perspective of information theory. A lot of the analysis shown in this paper involves the comparison of mutual information between different variables to understand how is it that fully connected neural networks learn.

\subsection{Background}
In this work, the authors use the notation of $x\in X$ to be the input patters, and $y\in Y$ to be the labels. Because neural networks is a way of showing the inputs as reduced representations, $T(X)$ denotes a representation (i.e. a layer), and the network tries to learn from an empirical sample drawn from an unknown joint distribution $P(X,Y)$.

In this work, the authors postulate a that deep neural network (DNN) generates a Markov chain representation by minimizing an empirical error over the weights, and the optimization is taken through a stochastic gradient descent (SGD) over a noisy estimate of the gradient of the empirical error, also known as backpropagation. This work also treats a whole layer $T$ as a single random variable, where an encoder $Pr(T|X)$ and a decoder $Pr(Y|T)$ characterize the layer, and an invertible transformation of representations is important to ensure that the transformation preserves information. Therefore, it is important quantify the representations with numbers that are invariant to any invertible reparametrization of T, and the mutual information $I(T,X)$ and $I(T,Y)$ is a fitting quantification. The mutual information is defined as

\begin{align*}
    I(X,Y) &= D_{KL} [Pr(x,y) \| Pr(X) Pr(Y)] \\
    &= \sum_{x,y} Pr(x,y) \log \left( \frac{Pr(x,y)}{Pr(x)Pr(y)} \right) \\
    &= \sum_{x,y} Pr(x,y) \log \left( \frac{Pr(x|y)}{Pr(x)} \right) \\
    &= H(X) - H(X|Y) \\
    &= \text{entropy of $X$} - \text{conditional entropy of X given Y}
\end{align*}

The mutual information quantifies the number of relevant bits that an input $X$ contains about label $Y$ on average. It is also important to mention that mutual information also has the commutative property.

It is also important to mention some properties with graphs and their mutual information characteristics. Given a Markov chain

$$ X \rightarrow Y \rightarrow Z$$

the data processing inequality states that
$$I(X,Y) \geq I(X,Z)$$

which means that information is generally lost when transmitted through a noisy channel.

\subsection{The Information Plane}
In this work, \cite{blackbox} postulate that the SGD has two phases that occur in this sequence
\begin{enumerate}
    \item Empirical error minimization (ERM)
    \item Representation compression
\end{enumerate}
where both of them have very different signal-to-noise ratios of the stochastic gradients at every layer. In the ERM, the gradient norms are much greater than the stochastic fluctuation, and in the compression, the fluctuation of gradients is much greater than the means, where the weights change as Weiner processes (i.e. random diffusions) with small influence on the error gradients. A lot of the work in \cite{blackbox} perform the analysis on the information plane, where the two axes are different mutual information measurements.

For a $K$-layer neural network, where $T_i$ represents the $i^{th}$ hidden layer as the single multivariate variable, any representation $T_i(X)$ is a mapping of $X$, characterized by $P(X,Y)$ or by the encoder distribution $P(T|X)$ and the decoder distribution $P(Y|T)$, ad the layers are mapped to $K$ monotonic connected paths in the plane, meaning that they satisfy the data processing inequality (DPI), where $\hat{Y}$ represents the predicted label,

$$I(X,Y) \geq I(T_1,Y) \geq I(T_2,Y) \geq \cdots \geq I(T_K,Y) \geq I(\hat{Y},Y)$$
$$H(X)   \geq I(X,T_1) \geq I(X,T_2) \geq \cdots \geq I(X,T_K) \geq I(X,\hat{Y})$$

Thus this work shows graphs plotting $I(X,T_i)$ vs $I(T_i,Y)$.

\subsection{Numerical Experiments}
\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.5\textwidth]{"./images/dnn_infoplane2".png}
    \caption{The network used for all of the numerical experiments}
    \label{fig::dnn_infoplane}
\end{center}
\end{figure}

For the numerical experiments, as shown in Figure \ref{fig::dnn_infoplane}, the authors used a fully connected architecture of the sequence (12,10,7,5,4,3,2) nodes where the inputs were binary, all of the activation functions were the hyperbolic tangent except for the last one with a sigmoid, a cross-entropy loss without any regularizer, and a stochastic gradient descent. In order to estimate the mutual information, there would be 50 randomized initializations, and the hyperbolic tangent values were binned in 30 intervals between -1 and 1.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.88\textwidth]{"./images/infoplane1_mod".png}
    \caption{The profile of the information place over of 50 randomly initialized networks, (left) when initialized, (center) after 400 epochs, and (right) after 9000 epochs. The dots in orange represent the last layer while the dots in green represent the initial layer.}
    \label{fig::dnn_infoplane}
\end{center}
\end{figure}

When observing the information plane profile of 50 randomly initialized networks after different epochs, as shown in Figure \ref{fig::dnn_infoplane}, it is possible to see that the layers first go through the empirical error minimization, where it allows the $I(T,Y)$ to greatly increase, and this phase occurs quickly. After the first phase, then the compression phase occurs, where the network starts to find a better compression on the input data, which is indicated by the reduction in $I(X,T)$.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.88\textwidth]{"./images/infoplane2".png}
    \caption{The evolution of the mutual information profile while using (left) 5\% of the data, (center) 45\% of the data, and (right) 85\% of the data}
    \label{fig::info_trainingdata1}
\end{center}
\end{figure}

Another approach that was to look at different training data sizes, which is shown in Figure \ref{fig::info_trainingdata1}. By doing so, it was clear to see that by having low training data size, the ERM phase remains relatively the same, but however, in the compression phase, the mutual information between the layers and the labels do decrease, meaning that it is overfitting to the small amount of data, which simplfies the layers' representations but loses the relevant information.

% \begin{figure}[ht]
% \begin{center}
%     \includegraphics[width=0.7\textwidth]{"./images/infoplane4".png}
%     \caption{The stable states of the information plane using different amounts of training data}
%     \label{fig::info_trainingdata2}
% \end{center}
% \end{figure}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.6\textwidth]{"./images/info_stochasticgrads_mod".png}
    \caption{The evolution of the normalized means and standard deviations of the weights and over the epochs. The first phase is described as the drift phase, with high signal-to-noise ratio, and the second phase is described as the diffusion phase, with low signal-to-noise ratios. The colors are represented by the weights of the respective layers shown in the inset illustration of the neural network.}
    \label{fig::info_sgd}
\end{center}
\end{figure}

The results from Figure \ref{fig::info_sgd} show the profiles of the normalized mean and standard deviations of the weights in a network. It starts off with a high signal-to-noise ratio, represented as the drift, and then it ends with a low signal-to-noise ratio, represented as the diffusion. During the drift, the network reduces the empirical error, while in the diffusion, it adds random noise to the weights, making it evolve as if it was a Wiener process. These processes can then be described by a Focker-Planck equation, where the stationary distribution attempts to maximize the conditional entroy, thus minimizing the mutual information.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.88\textwidth]{"./images/infoplane3".png}
    \caption{The information plane profiles with varying number of hidden layers, between 1-6.}
    \label{fig::info_varlayers}
\end{center}
\end{figure}

The final set of main results shown are in Figure \ref{fig::info_varlayers}, where the authors change the number of layers in the network, ranging from 1-6 hidden layers. This shows that the compression phase is significantly faster when the network is deeper, where the compression occurs faster on the deeper layers. Another remark they found is that, for this case, widening the hidden layers did not help much as all of the hidden layers eventually compress.


% \begin{figure}[ht]
% \begin{center}
%     \includegraphics[width=0.7\textwidth]{"./images/bottleneck".png}
%     \caption{The network used for all of the numerical experiments}
%     \label{fig::bottleneck}
% \end{center}
% \end{figure}
This paper was very interesting as it proposes a different methodology to understand how does information flow within neural networks. Although this is a topic that is very specific to these fully connected layered networks, it provides an entrypoint in a different perspective, which can be applied to different types of neural networks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Residual Networks as a Mean-Field Optimal Control Problem \cite{meanfield}}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.4\textwidth]{"./images/ResNets".png}
    \caption{An illustration of a residual network, where every layer is added to the following layer}
    \label{fig::resnets}
\end{center}
\end{figure}

\subsection{Background}
This paper was very interesting because it creates a optimal control formulation of a specific type of deep learning methodology. More specifically, it formulates a residual network, as shown in Figure \ref{fig::resnets}, as an optimal control problem. This is specifically because the feed forward propagation of a $T$-layer residual network is represented by

\begin{equation} \label{eq::resnet_form}
    x_{t+1} = x_t + f(x_t,\theta_t) \quad \quad \quad t \in \{0,\cdots,T-1\}
\end{equation}

where $x_0$ is the input to the network and $x_T$ is the final output, which is compared to a target $y_0$, which corresponds to $x_0$ via a loss function, and the goal is to train $\theta_0,\cdots,\theta_{T-1}$ such that $x_T$ is as close as possible to $y_0$. In this problem,$x_0 \in \mathbb{R}^d$ and $y_0 \in \mathbb{R}^l$ such that they are random variables jointly distributed according to the joint probability distribution $\mu_0 \mathbb{P}_{(x_0,y_0)}$. The set of admissible controls (i.e. training weights) is defined as $\Theta \subseteq \mathbb{R}^m$. The feed-forward dynamics $f$ maps $f:\mathbb{R}^d \times \Theta \rightarrow \mathbb{R}^d$, the terminal loss function $\Phi$ maps $\Phi : \mathbb{R}^d \times \mathbb{R}^{l} \rightarrow \mathbb{R}$, and the regularizer $L$ maps $L:\mathbb{R}^b \times \Theta \rightarrow \mathbb{R}$. The formulation for \ref{eq::resnet_form} is great, because, on can formulate a ordinary differential equation of it as

\begin{equation} \label{eq::resnet_ode}
    \dot{x}_t = f(x_t,\theta_t)
\end{equation}

Here, the bold-faced letters are used to describe a path-space quantity. For instance, $\bm{\theta} = \{ \theta_t | 0 \leq t \leq T \}$. Due to the equation \ref{eq::resnet_ode} being a fitting formulation for residual networks, a risk minimization problem in deep learning, it can then be formulated as a optimal control problem, either as a mean-field as well as an empirical
\begin{equation*}
\begin{dcases}
\text{Mean-Field: } & \inf_{\bm{\theta} \in L^{\infty} ([0,T], \Theta)} J(\bm{\theta}) = \mathbb{E}_{\mu_0} \left[ \Phi (x_T,y_0) + \int_0^T L(x_t,\theta_t) dt \right]  \text{   Subject to } \dot{x}_t = f(x_t,\theta_t) \\
\text{Empirical: }  & \inf_{\bm{\theta} \in L^{\infty} ([0,T], \Theta)} J_N(\bm{\theta}) = \frac{1}{N} \sum_{i=1}^N \left[ \Phi (x_T^i,y_0^i) + \int_0^T L(x_t^i,\theta_t) dt \right] \text{   Subject to } \dot{x}^i_t = f(x_t^i,\theta_t) \\
\end{dcases}
\end{equation*}

From this point on, $w \in \mathbb{R}^{d+l}$ will denote the concatenated $(x,y)$, $\bar{f}(w,\theta)$ represents the extended ($d+l$)-dimensional feed-forward function, $\bar{L(w,\theta)}$ represents the extended ($d+l$)-regularization loss, and $\bar{\Phi}(w)$ represents the terminal loss function. The gradient operators are depicted as $\nabla$, the Fr√©chet derivative on Banach spaces are depicted as $D$, $a\cdot b$ denotes the inner product of two Euclidean vectors, the $\| \cdot \|$ denotes the Euclidean norm, and the $\| \cdot \|$ denotes the absolute value. A fixed and sufficiently rich probability space $(\Omega, \mathcal{F}, \mathbb{P})$ is represented as $\Omega$, and the set of square integrable probability measures on the Euclidean space $\mathbb{R}^{d+l}$ is shown as $\mathcal{P}_2(\mathbb{R}^{d+l})$, which is equipped with the 2-Wasserstein distance

$$W_2(\mu,\nu) = \inf \left\{  \| X - Y \| _{L^2} | X,Y \in L^{2}(\Omega,\mathbb{R}^{d+l}) \text{ with } \mathbb{P}_X = \mu , \mathbb{P}_Y = \nu \right\}$$

With a measurable function $\psi : \mathbb{R}^{d+l} \rightarrow \mathbb{R}^q$ that is square integrable with respect to $\mu$, define
$$\langle \psi(\cdot) , \mu \rangle = \int_{\mathbb{R}^{d+l}} \psi(w) \mu(dw)$$

Additionally, from this point on, the notation for the infimum will be shorthanded as
$$ \inf_{\bm{\theta}} = \inf_{\bm{\theta} \in L^{\infty}([0,T],\Theta)}$$

\subsection{Mean-Field Dynamic Programming and HJB Equation}
In this section, the following assumptions are taken:
\begin{itemize}
    \item $f,L,\Phi$ are bounded, nad are Lipschitz continuous with respect to $x$, while the constants $f$ and $L$ are independent of $\theta $
    \item $\mu_0 \in \mathcal{P}(\mathbb{R}^{d+l})$
\end{itemize}

By using the formulation previously specified, the Hamilton-Jacobi-Bellman equation (HJB), which is subject to \ref{eq::resnet_ode}, can be rewritten as follows

\begin{align*}
J(t,\mu,\bm{\theta}) &= \mathbb{E}_{(x_t,y_0)\sim \mu} \left[ \Phi(x_T,y_0) + \int_t^T L(x_t,\theta_t) dt \right] \\
&= \langle \bar{\Phi}(\cdot),\mathbb{P}^{t,\mu,\bm{\theta}}_T \rangle + \int_t^T \langle \bar{L}(\cdot,\theta_s) , \mathbb{P}_s^{t,\mu,\bm{\theta}} \rangle ds
\end{align*}

With this, the value function $v^*(t,\mu)$ can be obtained with
\begin{equation*}
    v^*(t,\mu) = \inf_{\bm{\theta}} J(t,\mu,\bm{\theta}) \text{ where } J(\bm{\theta}) = v^*(0,\mu_0) \text{ by definition}
\end{equation*}

This value function is nice because it satisfies the dynamic programming principle, which states that for any optimal trajectory, if one start from any intermediate state in the trajectory, the remaining trajectory must be optimal. In other words

\begin{equation*} \label{eq::dp_optimality}
    v^*(t,\mu) = \inf_{\bm{\theta}} \left[ \int_t^{\hat{t}}  \langle \bar{L}(\cdot,\theta_s), \mathbb{P}_s^{t,\mu,\bm{\theta}} \rangle ds + v^* (\hat{t,\mathbb{P}_{\hat{t}}^{t,\mu,\bm{\theta}}}) \right]
\end{equation*}

Given such, the authors obtain a HJB in the Wasserstein Space by performing a formal Taylor Series expansion on the \ref{eq::dp_optimality}, yielding

\begin{equation}
    \begin{dcases}
        \frac{\partial v}{\partial t} + \inf_\theta \langle \frac{\partial v(t,\mu) (\cdot)}{\partial \mu} \cdot \bar{f}(\cdot,\theta) + \bar{L}(\cdot,\theta) , \mu \rangle = 0 & \text{ on } [0,T] \times \mathcal{P}_2(\mathbb{R}^{d+l}) \\
        v(T,\mu) = \langle \bar{\Phi}(\cdot),\mu \rangle & 0  \text{ on }  \mathcal{P}_2(\mathbb{R}^{d+l})
    \end{dcases}
    \tag{HJB}
\end{equation}

This then leads to the Proposition 3 in \cite{meanfield} that if $v$ is solution to (HJB) and there exists a $\theta'(t,\mu)$ that attains the infimum of (HJB), then $v=v^*$ and $\theta'$ is the optimal feedback control policy. This links the smooth solutions of HJB with the solutions of the mean-field optimal control problem, which means it also links to the population minimization problem in deep learning.

\subsection{Viscosity Solutions of HJB}
Many times, we cannot expect smooth solutions to the (HJB), and thus it's important to extend it to a weak solution by adding a viscosity to it in the Wasserstein space of the probability measures. By "lifting" the equations, it is possible to work in the Hilbert spaces instead of in the Wasserstein space. By defining Hamiltonian $\mathcal{H}$ as

\begin{equation*}
    \mathcal{H}(\xi, P) = \inf_\theta \mathbb{E} [P \cdot \bar{f}(\xi,\theta) + \bar{L}(\xi,\theta)]
\end{equation*}

The HJB can be set as

\begin{align*}
    \frac{\partial V}{\partial t} + \mathcal{H}(\xi,DV(t,\xi)) = 0 & \text{ on } [0,T] \times L^2(\Omega) \\
    V(T,\xi) = \mathbb{E}[\bar{\Phi}(\xi)] & \text{ on } L^2(\Omega)
\end{align*}

Using the above, \cite{meanfield} show that the value function $v^*(t,\mu)$ is a viscosity solution of (HJB), showing that it exists and it is unique. Thus, the optimal control policy can be used to define an optimal control as the solution to the learning problem. This then provides an entry point to understanding DL as a variational problem, whose solution is characterized by the HJB.

\subsection{Mean-Field Pontryagin's Maximum Principle}
In classical optimal control, one is able to find the best possible local control from taking a dynamical system from a start state to a goal state via Pontryagin's maximum principle (PMP). However, in the mean-field formulation, a common control parameter is shared by all of the input-target pairs, which are taken from the same distribution $\mu_0$. Therefore, there must exist a maximum principle in an average sense. Below are the assumptions for this section

\begin{itemize}
    \item The function $f$ is bounded, $f,L$ are continuous in $\theta$, and $f,L,\Phi$ are continuously differentiable with respect to $x$
    \item The distribution $\mu_0$ has a bounded support in $\mathbb{R}^d  \times \mathbb{R}^l$.
\end{itemize}

The Mean-Field PMP can be as, assuming that $\bm{\theta}^*$ be a solution of the mean-field formulation of the HJB such that $J(\bm{\theta}^*)$

\begin{align*}
    &\dot{x}_t^* = f(x_t^*, \theta_t^*) \text{ s.t. } x_t^* = x_0\\
    &\dot{p}_t^* = - \nabla_x H(x_t^*,p_t^*,\theta_t^*) \text{ s.t. } p_T^* = - \nabla_x \Phi(x_T^*, y_0) \\
    &\mathbb{E}_{\mu_0} [H(x_t^*,p_t^*,\theta_t^*)] \geq \mathbb{E}_{\mu_0} [H(x_t^*,p_t^*,\theta) ] \\
    &H(x,p,\theta) = p \cdot f(x,\theta) - L(x,\theta) = \text{ Hamiltonian function } H : \mathbb{R}^d \times \mathbb{R}^d \times \Theta \rightarrow \mathbb{R}
\end{align*}

The last equation is important because it is an unique feature for the PMP-type sttatements, where it makes optimal solutiosn globally maximize the Hamiltonian function. This is advantageous as it can deal with the case where the dynamics are not differentiable with respect to the controls (i.e. training weights), or when the optimal controls lie on the boundary of the set $\Theta$.

The authors further show that the PMP solutions can be derived from the HJB equation using the method of characteristics, where the derived Hamilton's equations \ref{eq::hamiltoneqns} are the characteristic equations of the HJB equation, thus the PMP pinpoints the necessary conditiona characteristic of HJB equation that originates from $\mu_0$ must satisfy.

\begin{align} \label{eq::hamiltoneqns}
    \dot{x}_t = f(\xi_t,\theta^*) \\
    \dot{p}_t = -\nabla_x f(x_t,\theta_t^*) p_t - \nabla_x L (x_t,\theta_t^*)
\end{align}

The authors conlude the paper proving that when a mean-field PMP is stable $\bm{\theta}^*$, then the sampled PMP $\bm{\theta}^N$ will have its solutions be near a stable solution from the mean-field PMP with probability

$$\mathbb{P}[|J(\bm{\theta}^N) - J(\bm{\theta}^*) \geq s] \leq 4 \exp\left( - \frac{Ns^2}{K_1 + K_2 s}\right)$$


Although this paper relied heavily on proofs, this paper presents a very important formulation for ResNets, establishing solid and proofed entrypoints to the understanding of the mathematical theory behind ResNets.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \section{Future}
