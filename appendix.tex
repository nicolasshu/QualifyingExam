\begin{appendices}
\chapter{Methodologies}
\section{Long Short-Term Memory and Gated Recurrent Unit} \label{app:LSTM_GRU}

\begin{center}
\begin{figure}[H]
    \raisebox{-0.5\height}{\includegraphics[width=0.45\textwidth]{"./images/LSTM".png}}
    \raisebox{-0.5\height}{\includegraphics[width=0.45\textwidth]{"./images/GRU".png}}
    \caption{A visual representation of the LSTM cell and the GRU cell}
    \label{app::LSTM_GRU}
\end{figure}
\end{center}

The LSTM and the GRU cell units are shown in Figure \ref{app::LSTM_GRU}, where $\sigma$ depicts the sigmoid function. The equations that govern the LSTM are
\begin{equation*}
\begin{cases}
\begin{pmatrix} i \\ f \\ o \\ g \end{pmatrix} = \begin{pmatrix} \sigma \\ \sigma \\ \sigma \\ \tanh \end{pmatrix} W \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix} \\
c_t f \cdot c_{t-1} + i \cdot g \\
h_t = o \cdot \tanh(c_t)
\end{cases}
\end{equation*}

The equations that govern the GRU are
\begin{equation*}
\begin{cases}
\text{Update Gate: } &  z_t = \sigma(W_z [h_{t-1},x_t]) \\
\text{Reset Gate: } & r_t = \sigma(W_r[h_{t-1},x_t]) \\
\text{Current Memory Content: } & \tilde{h}_t = \tanh(W \cdot [r_t \cdot h_{t-1}, x_t]) \\
\text{Final Memory: } & h_t = (1-z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t
\end{cases}
\end{equation*}

Although sigmoid functions are often frowned upon in the deep learning community due to the inevitable fact that often times, the gradients become zero at very high or low values, the sigmoids behave as switch knobs for the acceptance or rejection of previous information on the cell memory $c_t$ or on the hidden states $h_t$, which are coming from previous iterations. The vanishing gradient problem is avoided in LSTM and GRU or any network with forget gates through the concept of linear carousel.
% \section{Gaussian Mixture Models} \label{app:gmm}


\section{Spectral Clustering} \label{app:spectralclustering}

There are a lot of different clustering algorithms which may be used for unsupervised learning, but the issue is that, algorithms such as k-means or Gaussian mixture model clusterings assume that there is a compactness on the data. However, as seen in in Figure \ref{app::specclust}. Therefore, spectral clustering is a methodology in which allows one to cluster data based on proximity/connectivity.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.45\textwidth]{"./images/spectralclustering".png}
    \caption{A clustering result based on a (a) k-Means and (b) Spectral clustering}
    \label{app::specclust}
\end{center}
\end{figure}


In order to do a spectral clustering, first, one needs to compute an adjacency matrix $A$, where, for the $i^{th}$ node and $j^{th}$ node,
\begin{equation*}
    A_{ij} = \begin{cases}
                w_{ij}   \quad \text{: weight of the edge $(i,j)$}
                0        \quad \text{: if no edge exists between $i$ and $j$}
            \end{cases}
\end{equation*}

Then one needs to obtain the Laplacian matrix $L$, which is defined as $L=D-A$, where D is the diagonal matrix of degrees
\begin{align*}
    d_i &= \sum_j w_{ij} \\
    L_{ij} &= \begin{cases}
    d_i &\text{: if $i=j$} \\
    -w_{ij} &\text{if $(i,j)$ is an edge} \\
    0 &\text{if no edge between $i,j$}
    \end{cases}
\end{align*}

Once the Laplacian matrix has been computed, then one can compute the eigenvectors and eigenvalues of $L$. The number of eigenvalues that equal 0 defines the quantity of clusters that are likely to be created, and to compute the algebraic connectivity of the graph. Then, a k-means clustering algorithm can be applied on the elements of the eigenvectors in order to identify the clusters, and see which datapoint (i.e. node) belongs to which class.
% \section{Mel-Frequency Cepstrum Coefficients}
% \section{Linear Prediction Coefficients and its Derivatives}

\end{appendices}
