% \begin{center} \huge{Speech} \end{center}
% \section{Background}
% \section{Prior Work}
% \section{Preliminary Data}

\chapter{Speech Recognition in a Medical Environment}
\section{Background}

Automatic speech recognition (ASR) has been a topic that interested many from an early age. Many consider it to have started in the 1950s with Bell Labs' \textit{Audrey} \cite{voicerecog_history}, which was able to do a single-speaker digit recognition. Seventy years later, the ASR technologies have grown so that they are present within our personal homes, with Amazon Alexa and Google Home devices \cite{asr_at_home}, as they have become more affordable. As the speech technologies advance, they approach the concept of ubiquitous computing (often also known as ambient intelligence), which is highly desirable for many industries. Although most of the ASR systems available for commerce are trained in normal language and not medical language, one industry that could greatly benefit from ASR technologies is the health care system, given that the ASR system is able to understand medical language.

Health care is a system which has a very high demand and the services are required to be as detailed as possible due to various reasons, one of which is the concept that it is paramount for a hospital to have a clear and rich track of a patient's medical history. Currently, in order to maintain a patient's medical history, a physician sees multiple people during his/her working hours, and only after their shift is over, does (s)he sit down to write the medical notes. The lag in between seeing a patient and taking notes may sometimes go up to 8-10 hours, and then those notes are often inaccurate. Having an ASR system in a medical environment could greatly help in keeping track of a patient's medical history, where a physician could easily dictate the notes. In a hospital environment, however, such as in an intensive care unit (ICU), a physician who is trying to dictate notes may find him or herself in trouble, as in the ICU, there are multiple background sounds from machines, multiple people speaking, and a great amount of white noise.

Current companies have been creating ASR systems that can understand medical language and allows physicians to dictate their medical notes. One major company that has been "dominating" a lot of the market is Nuance, with their Dragon Medical system. Unfortunately, their dictation system is not yet capable of inferring punctuation marks and markup language onto the text, thus, in order to dictate a segment such as: "37-year-old female presents complainint of urinary frequency, urgency and dysuria along with hematuria and low-grade fever." needs to be dictated as:

\begin{center}
    "37-year-old female presents complainint of urinary frequency \textbf{comma} urgency and dysuria along with hematuria and low-grade fever \textbf{period}"
\end{center}

Although the system has very high accuracy results, the system works best when one is in a quiet environment, which is not always a realistic scenario. The goal of this project is to create a transcription engine that can recognize medical language in a noisy environment.

\section{Preliminary Data}

\subsection{Comparison of Different Commercial Engines}

Compare commercial ( + CMU)
Text->Voice-> Text
MIMIC II + 20kLeagues


\subsection{Grade Reading Level of Various Texts}
MIMIC II + 20kLeagues


\section{Automatic Speech Recognition Schematic for a Medical Setting}

In order to have a speech recognition system to be able to understand medical language, it is important to obtain a big picture of the project.


[INSERT FIGURE]


\textcolor{red}{\textit{As one can see from the diagram above, the sound signals would come to a microphone or an array of microphones, which would then be passed through a system that performs blind source separation on the sound signal. It is very possible that, in a hospital environment, there may be multiple sources, such as the heart rate monitor, the ventilator, the healthcare providers talking, and others. A blind source separation algorithm would ensure to separate the audio channel to multiple sources. Once the sources have been separated, there would be a classification methodology to identify the physician from the patient, and ...}}

\subsection{AI-Complete and ASR-Complete}
Although there have been many advancements in the speech technologies, especially through black boxes such as neural networks, and there currently exists very accurate engines to do voice recognition, it still remains to be an unsolved problem. The speech recognition problem is ASR-complete, which falls under the umbrella of AI-complete, which, by analogy of NP-completeness, means that the problem is hypothesized to deal with multiple parts of the AI world, and it cannot be solved by a single algorithm. Current technologies are not able to solve AI-complete problems, and they often require human computation. Since speech recognition is an AI-complete problem, there is still a lot of room for improvement in the fundamental problems that have not yet been solved, and although deep learning has recently shown promising results, deep learning neural networks (DNN) do not necessarily solve fundamental problems, as no one has a truthful understanding of how they work and behave. Therefore, there are a lot of possibilities for the formulation design of the framework (i.e. pipeline).

\section{Speech Enhancement}
Speech enhancement is one of fundamental problems of speech that has not yet been fully solved yet, as noise is subjective, and there isn't necessarily one specific way of performing a fully accurate noise reduction for any type of noise. As previously mentioned before, a ventilator and heart rate monitor are examples of background noise that do not necessarily have a Gaussian distribution (i.e. it is not a white noise), yet, for ASR, they are both irrelevant. There are a number of methodologies that have been created to attempt to solve the problem of speech enhancement, and they may potentially be useful to be implemented onto the pipeline.

\subsection{Recurrent Neural Networks for Noise Suppression}
The work on recurrent neural networks (RNN) was originally worked sequentially by \cite{rnn1} and \cite{rnn2}, and it finally came to be coined by \cite{rnn3}. They have shown to work very efficiently with time-series data as their cyclic nature allows them to adapt to incoming inputs in a sequential form, working often times better than convolutional neural networks (CNN), when dealing with time-series data. Other variants of RNNs have been created, which allow them to store "memory" for future cases, with Long Short-Term Memory (LSTM) \cite{lstm1}, which were then further expanded with gates for Gated Recurrent Units (GRU) \cite{gru1} specifically to allow such units to forget information, and a visual representation of LSTMs and GRUs are shown on Figure \ref{fig::LSTM_GRU}

\begin{figure}[H]
\begin{center}
    \raisebox{-0.5\height}{\includegraphics[width=0.45\textwidth]{"./images/LSTM".png}}
    \raisebox{-0.5\height}{\includegraphics[width=0.45\textwidth]{"./images/GRU".png}}
    \caption{A visual representation of the LSTM cell (Left) and the GRU cell (Right). The LSTM contains three representations of its states: the cell state/memory $c_t$, the hidden states $h_t$, and the input state $x_t$, all of which occurs at time $t$. On the GRU, there are the input state $x_t$, the hidden state $h_t$, the update gate $z_t$ and the reset gate $r_t$. The $\sigma$ block is used to represent a sigmoid function, and the $\tanh$ block is used to represent a hyperbolic tangent for both images. Please see \ref{app:LSTM_GRU} for the mathematical expressions of the cells above}
    \label{fig::LSTM_GRU}
\end{center}
\end{figure}

Because of the nature of the network being able to receive continuous sequential inputs, it becomes a very attractive model to potentially behave as an adaptive filter. Hence, members from the Xiph.Org Foundation, a non-profit organization, along with Mozilla have created a RNN architecture that is based on GRUs called RRNoise \cite{rrnoise}, which, after trained, is able to reduce a lot of the background noise of audio files, thus enhancing the signal. Below on Figure \ref{fig::RRNoisearch} is the topology of the RRNoise architecture, where it outputs voice activity detection (VAD) as well as the gains from the input features.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.65\textwidth]{"./images/RRNoise".png}
    \caption{The topology of the RRNoise network used to perform noise reduction}
    \label{fig::RRNoisearch}
\end{center}
\end{figure}

\subsection{Non-Negative Matrix Factorization}
Another very neat way of performing speech enhancement is via non-negative matrix factorization. Please skip to the Blind Source Separation's subsection on \hyperref[sec:NMF]{Non-Negative Matrix Factorization} \cite{spectralclustering}

\section{Blind Source Separation}
Given that a recording seldom contains one and only one speaker speaking at a time, an ASR may find itself troubled in detecting who is the speaker and who should be taken as the background. This is namely known as the Cocktail Party Effect. Its name originates from the idea that, in a cocktail party, there are multiple people speaking to one another. While a human is able to have a conversation with another, focusing on his/her friend while having all of the other speakers (i.e. sources) as background, an ASR system is not able to do so. Hence it is important to be able to separate a mixed signal to then be able to focus on a single individual and this is called blind source separation (BSS). There are two methodologies that are proposed to perform BSS: independent component analysis, and non-negative matrix factorization.


\subsection{Independent Component Analysis}
\subsection{Non-Negative Matrix Factorization (NMF) and Sparse NMF (SNMF)} \label{sec:NMF}
\subsubsection{Background}
One of the major drawbacks from any methodology in ICA is that it requires $N$ or more observations (i.e. microphone recordings) for $N$ sources (e.g. speakers, noise), which, depending on the circumstance, may be limiting. The method of non-negative matrix factorization (NMF), however, is able to perform BSS with very high accuracies, while using a single-channel recording. NMF originated in the field of chemometrics, where chemists were trying to identify contents of solutions via positive matrix factorization \cite{pmf} \cite{pmf_chem}. It was later further developed in \cite{nmf1} and algorithms were then better formulated in \cite{nmf2}.

The concept behind NMF is that a mixed signal should be able to be decomposed into its unmixed signals, under the assumption that all of the superpositioned signals are non-negatively added to one another. In other words, they are positively added, but their value may also be zero. Conceptually, this makes absolute sense. If one see the work in \cite{nmf1}, one will see that many of the components added are interpretations of existing physical features, such as eyebrows and mustache. It does not make sense to add a negative eyebrow to reconstruct one's face. Hence, in retrospect, it makes complete sense why this methodology was being used in chemometrics, as it does not make sense to add a negative sulfate ion to a solution.

One may ask how is this factorization different from other types of factorizations, such as principal component analysis (PCA). First, every factorization has different purposes; for instance, a Cholesky decomposition was specifically made to obtain lower computation complexity in order to invert matrices, where as PCA was made to obtain information about its principal orthogonal components. Second, when looking at the comparison between a signal reconstruction or separation from an NMF and from PCA, PCA poses a lot of problems. PCA only allows for the feature space to represent vectors that are orthogonal, which is not always the case. In the sense of speech, it is almost impossible for one speech signal to be orthogonal to another one. PCA also assumes that the probability distribution of the data follows a multivariate Gaussian, which again is not always the case. A NNF is very promising as it performs a matrix decomposition, which is very desirable for computational reconstruction in the last step of the BSS, and it is able to attain reconstructions just as detailed as some of the other reconstruction techniques, plus one is allowed to determine how many components to use.

Here, the notation follows the notations shown in \cite{singlechannel}.

Consider the magnitude of a spectrogram of a mixed speech signal $\bm{Y} \in \mathbb{R}^{M\times N}$, which is a summation of $R$ source signals (i.e. $\bm{Y} = \sum_{i=1}^{R} \bm{Y}_i$). The spectrogram can be sparsely represented in an overcomplete basis as

$$\bm{Y} = \bm{D} \bm{H}$$

where $\bm{D} \in \mathbb{R}^{M \times R}$ represents a compendium of dictionaries with $R$ columns, which can be chosen by the user, and $\bm{H} \in \mathbb{R}^{R \times N}$ represents a code matrix, which is sparse and it contains code matrices associated with the dictionaries in $\bm{D}$. The matrix $\bm{D}$ and $\bm{H}$ can be represented as
\begin{equation*}
    \bm{D} = \begin{bmatrix}
    | & | & & | \\
    | & | & & | \\
    | & | & & | \\
    \bm{D}_1 & \bm{D}_2 & \cdots & \bm{D}_R \\
    | & | & & | \\
    | & | & & | \\
    | & | & & | \\
    \end{bmatrix}
\end{equation*}

\begin{equation*}
\bm{H} =
    \begin{bmatrix}
        --- & --- & --- & \bm{H}_1 & --- & --- & --- \\
        --- & --- & --- & \bm{H}_1 & --- & --- & --- \\
            &     &     & \vdots   &     &     &     \\
        --- & --- & --- & \bm{H}_R & --- & --- & --- \
    \end{bmatrix}
\end{equation*}

A simple version of how a NMF would work is shown in Figure \ref{fig::nmf_simple}, where the figure below was inspired on the works from \cite{singlechannel}.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.85\textwidth]{"./images/nmf1".png}
    \caption{A very simple schematic of a non-negative matrix factorization, where the matrix on the left represents the mixed signal, the matrix on the center represents the dictionary matrix, and the matrix on the right represents the coding matrix}
    \label{fig::nmf_simple}
\end{center}
\end{figure}

There are two different ways to obtain the desired decomposition, and that is based on what optimization approach one wishes to take. One approach is to minimize the squared Euclidean-based Frobenius norm, $\| \bm{Y} - \bm{D}\bm{H} \|^2_F $, and another approach is to minimize the divergence, $\mathcal{D}(\bm{Y}\|\bm{DH})$, both cases with respect to $\bm{W}$ and $\bm{H}$, subject to the contraints that $\bm{D},\bm{H} \geq 0$ \cite{nmf2}. Both of their definitions are shown below, where the subscript $i$ represents the $i^{th}$ row, and the $j$ represents the $j^{th}$ column. Notice that the divergence measurement below is not the Kullback-Leibler (KL) divergence, but it does converge to the KL divergence if $\sum_{ij} A_{ij} = \sum_{ij} B_{ij} = 1$s

\begin{equation*}
\begin{dcases}
\text{Norm:} & \|A - B \|^2_F = \sum_{i,j} (A_{ij} - B_{ij})^2 \\
\text{Divergence:} & \mathcal{D}(A\|B) = \sum_{i,j} \left(A_{ij} \log \left( \frac{A_{ij}}{{B_{ij}}}  - A_{ij} + B_{ij} \right) \right)
\end{dcases}
\end{equation*}

Because this is an iterative algorithm, if one follows the norm minimization, the update rules are:

\begin{codebox}
NMF Algorithm for Norm Minimization
\begin{align*}
    &1. \text{Initialize }\bm{D,H} \geq 0 \\
    &2. \bm{H}_{ij} \leftarrow \bm{H}_{ij} \frac{ (\bm{D}^T \bm{Y})_{ij} }{ (\bm{D}^T \bm{D} \bm{H})_{ij} } \\
    &3. \bm{D}_{ij} \leftarrow \bm{D}_{ij} \frac{ (\bm{Y} \bm{H}^T)_{ij} }{ (\bm{D}\bm{H}\bm{H}^T)_{ij} } \\
    &4. \text{Repeat 2,3 until stability}
\end{align*}
\end{codebox}

If one follows the divergence minimization, the update rules are
\begin{codebox}
NMF Algorithm for Divergence Minimization
\begin{align*}
    &1. \text{Initialize }\bm{D,H} \geq 0 \\
    &2. \bm{H}_{ij} \leftarrow \bm{H}_{ij} \frac{ \sum_k \bm{D}_{ki} \bm{Y}_{kj} / (\bm{DH})_{kj} }{ \sum_l \bm{D}_{li} } \\
    &3. \bm{D}_{ij} \leftarrow \bm{D}_{ij} \frac{ \sum_k \bm{H}_{jk} \bm{Y}_{ik} / (\bm{DH})_{ik} }{ \sum_l \bm{H}_{jl} } \\
    &4. \text{Repeat 2,3 until stability}
\end{align*}
\end{codebox}

If one performs an NMF, a visual representation is shown on Figure \ref{fig::nmf_example}, where the figure below was inspired on the works from \cite{singlechannel}.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.85\textwidth]{"./images/nmf2".png}
    \caption{A visual example of an NMF result, with two sources being separated.}
    \label{fig::nmf_example}
\end{center}
\end{figure}


The algorithms for NMF were further improved by \cite{sparsenmf}, where the authors created a Sparse NMF. Their work was inspired by the basic NMF problems that, when one had overcomplete dictionaries, there was no well defined solution such that you would obtain sparse solutions. Thus, they wanted to make sure to obtain sparse solutions, especially on the coding matrix. To tune the sparsity measurement, the authors simply applied a L1 regularization based on the coding matrix coefficients on the cost functional of the norm, and called it a Sparse NMF. In other words, the cost (or energy) functional that originally was $\mathcal{E} = \| Y - \bar{\bm{D}} \bm{H} \|_F^2$ became

$$\mathcal{E} = \| \bm{Y} - \bar{\bm{D}} \bm{H} \|_F^2 + \underbrace{\lambda \sum_{i,j} H_{ij}}_{L1\text{ regularization}} \quad s.t. \quad \bm{D},\bm{H} \geq 0$$

where $\bar{\bm{D}}$ still follows the notation of \cite{singlechannel}, representing a column-wise normalized dictionary matrix, and $\lambda$ is the parameter to control the degree of sparsity. To solve this optimization problem, if one was to set $\bm{R} = \bm{DH}$, then one would obtain the following algorithm. Note, for this one case, that the dot sign $\cdot$ and and division operator $\frac{a}{b}$ represent point-wise multiplication and division

\begin{codebox}
Sparse NMF Algorithm for Frobenius Norm L1 Regularized Minimization
\begin{align*}
    &1. \text{Initialize }\bm{D,H} \geq 0 \\
    &2. \bm{H}_{ij} \leftarrow \bm{H}_{ij} \cdot \frac{ \bm{Y}_i^T \bar{\bm{D}}_j }{  \bm{R}_i^T  \bar{\bm{D}}_j + \lambda } \quad \quad \quad \quad \quad \quad \quad \quad \in \mathbb{R}^1 \cdot \frac{\mathbb{R}^{1\times M} \mathbb{R}^{M \times 1}}{\mathbb{R}^{1\times M} \mathbb{R}^{M \times 1} + \mathbb{R}^{1}} \\
    &3. \bm{D}_{ij} \leftarrow \bm{D}_{ij} \cdot \frac{ \sum_i \bm{H}_{ij} [ \bm{Y}_i + (\bm{R}_i^T \bar{\bm{D}}_j) \bar{\bm{D}}_j] }{ \sum_i \bm{H}_ij [\bm{R}_i + (\bm{Y}_i \bar{\bm{D}}_j) \bar{\bm{D}}_j ]} \quad \quad \quad \quad \in \mathbb{R}^{M\times 1} \cdot \frac{  \sum_i \mathbb{R}^{1} + (\mathbb{R}^{1\times M} \mathbb{R}^{M\times 1})\mathbb{R}^{M\times 1}  }{ \sum_i \mathbb{R}^{1} [\mathbb{R}^{M \times 1} + (\mathbb{R}^{1\times M} \mathbb{R}^{M\times 1}) \mathbb{R}^{M\times 1}]  }\\
    &4. \text{Repeat 2,3 until stability}
\end{align*}
\end{codebox}

All of the algorithms shown above are the steps necessary to train the algorithm, where $\bm{Y}$ contains all of the training data. In order to actually perform a speech reconstruction, one should simply perform the algorithms above, but to keep the dictionary matrix $\bm{D}$ fixed and update only the code matrix $\bm{H}$. The speech is then separated by conputing the reconstruction of parts of the sparse decomposition. In other words:
\begin{enumerate}
    \item Apply NMF or Sparse NMF to learn the dictionaries of individual speakers
    \item To separate the mixtures, keep $\bm{D}$ fixed, and only update $\bm{H}$
    \item Reconstruct the signal by using the desired parts of the decomposition
\end{enumerate}


\subsubsection{Aproaches to Learn Dictionaries}
As shown in \cite{singlechannel}, there are multiple approaches to perform BSS with NMF or SNMF: unsupervised approach and segmenting the training data and solving multiple NMF problems

In the unsupervised approach, one would compute the SNMF over a very large dataset matrix $\bm{Y}$ of a single speaker to obtain a single dictionary. This, however, may take a lot of computation time. The second approach, used in \cite{singlechannel} involved segmenting the training data according to phoneme labels obtained by a speech recognition software with hidden-Markov models (HMM) for a single speaker, and creating a sparse dictionary for each phoneme, and finally , a final dictionary would be constructed by the concatenation of individual phoneme dictionaries. In order to create the dictionary of each phoneme, each type of phoneme would be concatenated together, to create that phoneme's dictionary, and then, the phonemes dictionaries would be concatenated together to create a dictionary that is representative of the speaker. Figure \ref{fig::speaker_dict} shows an illustration on how to create the dictionary.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.45\textwidth]{"./images/speaker_dict".png}
    \caption{An illustration on how to create a speaker's dictionary $\bm{D}_i$}
    \label{fig::speaker_dict}
\end{center}
\end{figure}

\subsubsection{NMF for Speech Enhancement}
The works from \cite{nmf_denoising} have shown that it is also possible to perform speech enhancement on a speech signal via non-negative matrix factorization. An illustration on Figure \ref{fig::nmf_denoising}, inspired by the images on \cite{nmf_denoising}, shows the procedure. It is possible to generate the dictionary matrix $\bm{D}$ and the coding matrix $\bm{H}$ from a noisy signal, however one would not know which of the subdictionaries $\bm{D}_i$ or the code submatrices $\bm{H}_i$ represent the ones associated with the speech and with the noise. Through spectral clustering (for technical details, see \ref{app:spectralclustering}), it is possible to group the subparts of the dictionary matrix and the code matrix so that it is differentiated between speech and noise. Given that the reconstruction process is based on linear algebra rules, it is then possible to simply multiply the dictionary matrix related to speech $\bm{D}_{speech}$ and the code matrix related to speech $\bm{H}_{speech}$ to obtain the cleaned speech.

Of course this solution is not guaranteed to remove every single noise source due to the possible stochasticity that is propagated to the eigenvectors of the Laplacian, and the fact that the solution is not unique, which is based on the initialization of the centroids of the k-means algorithm, but it could be a good fundamental method to implement.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=0.4\textwidth]{"./images/nmf_denoising".png}
    \caption{An illustration on how to use non-negative matrix factorization to denoise a signal, where $\bm{Y}$ represents the noisy signal, $\bm{D}$ represents the dictionary matrix obtained through NMF, and $\bm{H}$ represents the code matrix, also attained through the NMF. }
    \label{fig::nmf_denoising}
\end{center}
\end{figure}


% \section{Speaker Diarisation}
% \section{Speaker Verification}
% \section{Speech Recognition}
% \subsection{Hidden-Markov Models and Gaussian Mixture Models} \label{sec:hmm}
% \subsection{Long Short Term Memory Networks}
%
% \section{Features from Speech}
