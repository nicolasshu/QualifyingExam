% \begin{center} \huge{Speech} \end{center}
% \section{Background}
% \section{Prior Work}
% \section{Preliminary Data}


\section{Background}

Automatic speech recognition (ASR) has been a topic that interested many from an early age. Many consider it to have started in the 1950s with Bell Labs' \textit{Audrey} \cite{voicerecog_history}, which was able to do a single-speaker digit recognition. Seventy years later, the ASR technologies have grown so that they are present within our personal homes, with Amazon Alexa and Google Home devices \cite{asr_at_home}, as they have become more affordable. As the speech technologies advance, they approach the concept of ubiquitous computing (often also known as ambient intelligence), which is highly desirable for many industries. Although most of the ASR systems available for commerce are trained in normal language and not medical language, one industry that could greatly benefit from ASR technologies is the health care system, given that the ASR system is able to understand medical language.

Health care is a system which has a very high demand and the services are required to be as detailed as possible due to various reasons, one of which is the concept that it is paramount for a hospital to have a clear and rich track of a patient's medical history. Currently, in order to maintain a patient's medical history, a physician sees multiple people during his/her working hours, and only after their shift is over, does (s)he sit down to write the medical notes. The lag in between seeing a patient and taking notes may sometimes go up to 8-10 hours, and then those notes are often inaccurate. Having an ASR system in a medical environment could greatly help in keeping track of a patient's medical history, where a physician could easily dictate the notes. In a hospital environment, however, such as in an intensive care unit (ICU), a physician who is trying to dictate notes may find him or herself in trouble, as in the ICU, there are multiple background sounds from machines, multiple people speaking, and a great amount of white noise.

Current companies have been creating ASR systems that can understand medical language and allows physicians to dictate their medical notes. One major company that has been "dominating" a lot of the market is Nuance, with their Dragon Medical system. Unfortunately, their dictation system is not yet capable of inferring punctuation marks and markup language onto the text, thus, in order to dictate a segment such as: "37-year-old female presents complainint of urinary frequency, urgency and dysuria along with hematuria and low-grade fever." needs to be dictated as:

\begin{center}
    "37-year-old female presents complainint of urinary frequency \textbf{comma} urgency and dysuria along with hematuria and low-grade fever \textbf{period}"
\end{center}

Although the system has very high accuracy results, the system works best when one is in a quiet environment, which is not always a realistic scenario. The goal of this project is to create a transcription engine that can recognize medical language in a noisy environment.

\section{Preliminary Data}

\subsection{Comparison of Different Commercial Engines}

Compare commercial ( + CMU)
Text->Voice-> Text
MIMIC II + 20kLeagues


\subsection{Grade Reading Level of Various Texts}
MIMIC II + 20kLeagues


\section{Automatic Speech Recognition Schematic for a Medical Setting}

In order to have a speech recognition system to be able to understand medical language, it is important to obtain a big picture of the project.


[INSERT FIGURE]


\textcolor{red}{\textit{As one can see from the diagram above, the sound signals would come to a microphone or an array of microphones, which would then be passed through a system that performs blind source separation on the sound signal. It is very possible that, in a hospital environment, there may be multiple sources, such as the heart rate monitor, the ventilator, the healthcare providers talking, and others. A blind source separation algorithm would ensure to separate the audio channel to multiple sources. Once the sources have been separated, there would be a classification methodology to identify the physician from the patient, and ...}}

Although there have been many advancements in the speech technologies and there currently exists very accurate engines to do voice recognition, it still remains to be an unsolved problem, as there is always room for improvement. Therefore, there are a lot of possibilities for the formulation design of the framework (i.e. pipeline).

\section{Speech Enhancement}
\subsection{Convolutional Neural Networks for Noise Suppression}
\subsection{Non-Negative Matrix Factorization}
Please skip to the Blind Source Separation's subsection on \hyperref[sec:NMF]{Non-Negative Matrix Factorization}
\section{Blind Source Separation}
\subsection{Independent Component Analysis}
\subsection{Non-Negative Matrix Factorization (NMF) and Sparse NMF} \label{sec:NMF}

\section{Speaker Diarisation}
\section{Speaker Verification}
\section{Speech Recognition}
\subsection{Hidden-Markov Models and Gaussian Mixture Models}
\subsection{Long Short Term Memory Networks}
