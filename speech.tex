% \begin{center} \huge{Speech} \end{center}
% \section{Background}
% \section{Prior Work}
% \section{Preliminary Data}

\chapter{Speech Recognition in a Medical Environment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}

Automatic speech recognition (ASR) has been a topic that interested many from an early age. Many consider it to have started in the 1950s with Bell Labs' \textit{Audrey} \cite{voicerecog_history}, which was able to do a single-speaker digit recognition. Seventy years later, the ASR technologies have grown so that they are present within our personal homes, with Amazon Alexa and Google Home devices \cite{asr_at_home}, as they have become more affordable. As the speech technologies advance, they approach the concept of ubiquitous computing (often also known as ambient intelligence), which is highly desirable for many industries. Although most of the ASR systems available for commerce are trained in normal language and not medical language, one industry that could greatly benefit from ASR technologies is the health care system, given that the ASR system is able to understand medical language.

Health care is a system which has a very high demand and the services are required to be as detailed as possible due to various reasons, one of which is the concept that it is paramount for a hospital to have a clear and rich track of a patient's medical history. Currently, in order to maintain a patient's medical history, a physician sees multiple people during his/her working hours, and only after their shift is over, does (s)he sit down to write the medical notes. The lag in between seeing a patient and taking notes may sometimes go up to 8-10 hours, and then those notes are often inaccurate. Having an ASR system in a medical environment could greatly help in keeping track of a patient's medical history, where a physician could easily dictate the notes. In a hospital environment, however, such as in an intensive care unit (ICU), a physician who is trying to dictate notes may find him or herself in trouble, as in the ICU, there are multiple background sounds from machines, multiple people speaking, and a great amount of white noise.

Current companies have been creating ASR systems that can understand medical language and allows physicians to dictate their medical notes. One major company that has been "dominating" a lot of the market is Nuance, with their Dragon Medical system. Unfortunately, their dictation system is not yet capable of inferring punctuation marks and markup language onto the text, thus, in order to dictate a segment such as: "37-year-old female presents complainint of urinary frequency, urgency and dysuria along with hematuria and low-grade fever." needs to be dictated as:

\begin{center}
    "37-year-old female presents complainint of urinary frequency \textbf{comma} urgency and dysuria along with hematuria and low-grade fever \textbf{period}"
\end{center}

Although the system has very high accuracy results, the system works best when one is in a quiet environment, which is not always a realistic scenario. The goal of this project is to create a transcription engine that can recognize medical language in a noisy environment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminary Data}

\subsection{Comparison of Different Commercial Engines}

Compare commercial ( + CMU)
Text->Voice-> Text
MIMIC II + 20kLeagues


\subsection{Grade Reading Level of Various Texts}
MIMIC II + 20kLeagues

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Automatic Speech Recognition Schematic for a Medical Setting}

In order to have a speech recognition system to be able to understand medical language, it is important to obtain a big picture of the project.


[INSERT FIGURE]


\textcolor{red}{\textit{As one can see from the diagram above, the sound signals would come to a microphone or an array of microphones, which would then be passed through a system that performs blind source separation on the sound signal. It is very possible that, in a hospital environment, there may be multiple sources, such as the heart rate monitor, the ventilator, the healthcare providers talking, and others. A blind source separation algorithm would ensure to separate the audio channel to multiple sources. Once the sources have been separated, there would be a classification methodology to identify the physician from the patient, and ...}}

\subsection{AI-Complete and ASR-Complete}
Although there have been many advancements in the speech technologies, especially through black boxes such as neural networks, and there currently exists very accurate engines to do voice recognition, it still remains to be an unsolved problem. The speech recognition problem is ASR-complete, which falls under the umbrella of AI-complete, which, by analogy of NP-completeness, means that the problem is hypothesized to deal with multiple parts of the AI world, and it cannot be solved by a single algorithm. Current technologies are not able to solve AI-complete problems, and they often require human computation. Since speech recognition is an AI-complete problem, there is still a lot of room for improvement in the fundamental problems that have not yet been solved, and although deep learning has recently shown promising results, deep learning neural networks (DNN) do not necessarily solve fundamental problems, as no one has a truthful understanding of how they work and behave. Therefore, there are a lot of possibilities for the formulation design of the framework (i.e. pipeline).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Speech Enhancement}
Speech enhancement is one of fundamental problems of speech that has not yet been fully solved yet, as noise is subjective, and there isn't necessarily one specific way of performing a fully accurate noise reduction for any type of noise. As previously mentioned before, a ventilator and heart rate monitor are examples of background noise that do not necessarily have a Gaussian distribution (i.e. it is not a white noise), yet, for ASR, they are both irrelevant. There are a number of methodologies that have been created to attempt to solve the problem of speech enhancement, and they may potentially be useful to be implemented onto the pipeline.

\subsection{Recurrent Neural Networks for Noise Suppression} \label{sec:rnn}
The work on recurrent neural networks (RNN) was originally worked sequentially by \cite{rnn1} and \cite{rnn2}, and it finally came to be coined by \cite{rnn3}. They have shown to work very efficiently with time-series data as their cyclic nature allows them to adapt to incoming inputs in a sequential form, working often times better than convolutional neural networks (CNN), when dealing with time-series data. Other variants of RNNs have been created, which allow them to store "memory" for future cases, with Long Short-Term Memory (LSTM) \cite{lstm1}, which were then further expanded with gates for Gated Recurrent Units (GRU) \cite{gru1} specifically to allow such units to forget information, and a visual representation of LSTMs and GRUs are shown on Figure \ref{fig::LSTM_GRU}

\begin{figure}[H]
\begin{center}
    \raisebox{-0.5\height}{\includegraphics[width=0.45\textwidth]{"./images/LSTM".png}}
    \raisebox{-0.5\height}{\includegraphics[width=0.45\textwidth]{"./images/GRU".png}}
    \caption{A visual representation of the LSTM cell (Left) and the GRU cell (Right). The LSTM contains three representations of its states: the cell state/memory $c_t$, the hidden states $h_t$, and the input state $x_t$, all of which occurs at time $t$. On the GRU, there are the input state $x_t$, the hidden state $h_t$, the update gate $z_t$ and the reset gate $r_t$. The $\sigma$ block is used to represent a sigmoid function, and the $\tanh$ block is used to represent a hyperbolic tangent for both images. Please see \ref{app:LSTM_GRU} for the mathematical expressions of the cells above}
    \label{fig::LSTM_GRU}
\end{center}
\end{figure}

Because of the nature of the network being able to receive continuous sequential inputs, it becomes a very attractive model to potentially behave as an adaptive filter. Hence, members from the Xiph.Org Foundation, a non-profit organization, along with Mozilla have created a RNN architecture that is based on GRUs called RRNoise \cite{rrnoise}, which, after trained, is able to reduce a lot of the background noise of audio files, thus enhancing the signal. Below on Figure \ref{fig::RRNoisearch} is the topology of the RRNoise architecture, where it outputs voice activity detection (VAD) as well as the gains from the input features.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.65\textwidth]{"./images/RRNoise".png}
    \caption{The topology of the RRNoise network used to perform noise reduction. This figure is taken from \cite{rrnoise}}
    \label{fig::RRNoisearch}
\end{center}
\end{figure}

\subsection{Non-Negative Matrix Factorization}
Another very neat way of performing speech enhancement is via non-negative matrix factorization. Please skip to the Blind Source Separation's subsection on \hyperref[sec:NMF]{Non-Negative Matrix Factorization} \cite{spectralclustering}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Blind Source Separation}
Given that a recording seldom contains one and only one speaker speaking at a time, an ASR may find itself troubled in detecting who is the speaker and who should be taken as the background. This is namely known as the Cocktail Party Effect. Its name originates from the idea that, in a cocktail party, there are multiple people speaking to one another. While a human is able to have a conversation with another, focusing on his/her friend while having all of the other speakers (i.e. sources) as background, an ASR system is not able to do so. Hence it is important to be able to separate a mixed signal to then be able to focus on a single individual and this is called blind source separation (BSS). There are two methodologies that are proposed to perform BSS: independent component analysis, and non-negative matrix factorization.


\subsection{Independent Component Analysis}
\subsection{Non-Negative Matrix Factorization (NMF) and Sparse NMF (SNMF)} \label{sec:NMF}
\subsubsection{Background}
One of the major drawbacks from any methodology in ICA is that it requires $N$ or more observations (i.e. microphone recordings) for $N$ sources (e.g. speakers, noise), which, depending on the circumstance, may be limiting. The method of non-negative matrix factorization (NMF), however, is able to perform BSS with very high accuracies, while using a single-channel recording. NMF originated in the field of chemometrics, where chemists were trying to identify contents of solutions via positive matrix factorization \cite{pmf} \cite{pmf_chem}. It was later further developed in \cite{nmf1} and algorithms were then better formulated in \cite{nmf2}.

The concept behind NMF is that a mixed signal should be able to be decomposed into its unmixed signals, under the assumption that all of the superpositioned signals are non-negatively added to one another. In other words, they are positively added, but their value may also be zero. Conceptually, this makes absolute sense. If one see the work in \cite{nmf1}, one will see that many of the components added are interpretations of existing physical features, such as eyebrows and mustache. It does not make sense to add a negative eyebrow to reconstruct one's face. Hence, in retrospect, it makes complete sense why this methodology was being used in chemometrics, as it does not make sense to add a negative sulfate ion to a solution.

One may ask how is this factorization different from other types of factorizations, such as principal component analysis (PCA). First, every factorization has different purposes; for instance, a Cholesky decomposition was specifically made to obtain lower computation complexity in order to invert matrices, where as PCA was made to obtain information about its principal orthogonal components. Second, when looking at the comparison between a signal reconstruction or separation from an NMF and from PCA, PCA poses a lot of problems. PCA only allows for the feature space to represent vectors that are orthogonal, which is not always the case. In the sense of speech, it is almost impossible for one speech signal to be orthogonal to another one. PCA also assumes that the probability distribution of the data follows a multivariate Gaussian, which again is not always the case. A NNF is very promising as it performs a matrix decomposition, which is very desirable for computational reconstruction in the last step of the BSS, and it is able to attain reconstructions just as detailed as some of the other reconstruction techniques, plus one is allowed to determine how many components to use.

Here, the notation follows the notations shown in \cite{singlechannel}.

Consider the magnitude of a spectrogram of a mixed speech signal $\bm{Y} \in \mathbb{R}^{M\times N}$, which is a summation of $R$ source signals (i.e. $\bm{Y} = \sum_{i=1}^{R} \bm{Y}_i$). The spectrogram can be sparsely represented in an overcomplete basis as

$$\bm{Y} = \bm{D} \bm{H}$$

where $\bm{D} \in \mathbb{R}^{M \times R}$ represents a compendium of dictionaries with $R$ columns, which can be chosen by the user, and $\bm{H} \in \mathbb{R}^{R \times N}$ represents a code matrix, which is sparse and it contains code matrices associated with the dictionaries in $\bm{D}$. The matrix $\bm{D}$ and $\bm{H}$ can be represented as
\begin{equation*}
    \bm{D} = \begin{bmatrix}
    | & | & & | \\
    | & | & & | \\
    | & | & & | \\
    \bm{D}_1 & \bm{D}_2 & \cdots & \bm{D}_R \\
    | & | & & | \\
    | & | & & | \\
    | & | & & | \\
    \end{bmatrix}
\end{equation*}

\begin{equation*}
\bm{H} =
    \begin{bmatrix}
        --- & --- & --- & \bm{H}_1 & --- & --- & --- \\
        --- & --- & --- & \bm{H}_1 & --- & --- & --- \\
            &     &     & \vdots   &     &     &     \\
        --- & --- & --- & \bm{H}_R & --- & --- & --- \
    \end{bmatrix}
\end{equation*}

A simple version of how a NMF would work is shown in Figure \ref{fig::nmf_simple}, where the figure below was inspired on the works from \cite{singlechannel}.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.85\textwidth]{"./images/nmf1".png}
    \caption{A very simple schematic of a non-negative matrix factorization, where the matrix on the left represents the mixed signal, the matrix on the center represents the dictionary matrix, and the matrix on the right represents the coding matrix}
    \label{fig::nmf_simple}
\end{center}
\end{figure}

There are two different ways to obtain the desired decomposition, and that is based on what optimization approach one wishes to take. One approach is to minimize the squared Euclidean-based Frobenius norm, $\| \bm{Y} - \bm{D}\bm{H} \|^2_F $, and another approach is to minimize the divergence, $\mathcal{D}(\bm{Y}\|\bm{DH})$, both cases with respect to $\bm{W}$ and $\bm{H}$, subject to the contraints that $\bm{D},\bm{H} \geq 0$ \cite{nmf2}. Both of their definitions are shown below, where the subscript $i$ represents the $i^{th}$ row, and the $j$ represents the $j^{th}$ column. Notice that the divergence measurement below is not the Kullback-Leibler (KL) divergence, but it does converge to the KL divergence if $\sum_{ij} A_{ij} = \sum_{ij} B_{ij} = 1$s

\begin{equation*}
\begin{dcases}
\text{Norm:} & \|A - B \|^2_F = \sum_{i,j} (A_{ij} - B_{ij})^2 \\
\text{Divergence:} & \mathcal{D}(A\|B) = \sum_{i,j} \left(A_{ij} \log \left( \frac{A_{ij}}{{B_{ij}}}  - A_{ij} + B_{ij} \right) \right)
\end{dcases}
\end{equation*}

Because this is an iterative algorithm, if one follows the norm minimization, the update rules are:

\begin{codebox}
NMF Algorithm for Norm Minimization
\begin{align*}
    &1. \text{Initialize }\bm{D,H} \geq 0 \\
    &2. \bm{H}_{ij} \leftarrow \bm{H}_{ij} \frac{ (\bm{D}^T \bm{Y})_{ij} }{ (\bm{D}^T \bm{D} \bm{H})_{ij} } \\
    &3. \bm{D}_{ij} \leftarrow \bm{D}_{ij} \frac{ (\bm{Y} \bm{H}^T)_{ij} }{ (\bm{D}\bm{H}\bm{H}^T)_{ij} } \\
    &4. \text{Repeat 2,3 until stability}
\end{align*}
\end{codebox}

If one follows the divergence minimization, the update rules are
\begin{codebox}
NMF Algorithm for Divergence Minimization
\begin{align*}
    &1. \text{Initialize }\bm{D,H} \geq 0 \\
    &2. \bm{H}_{ij} \leftarrow \bm{H}_{ij} \frac{ \sum_k \bm{D}_{ki} \bm{Y}_{kj} / (\bm{DH})_{kj} }{ \sum_l \bm{D}_{li} } \\
    &3. \bm{D}_{ij} \leftarrow \bm{D}_{ij} \frac{ \sum_k \bm{H}_{jk} \bm{Y}_{ik} / (\bm{DH})_{ik} }{ \sum_l \bm{H}_{jl} } \\
    &4. \text{Repeat 2,3 until stability}
\end{align*}
\end{codebox}

If one performs an NMF, a visual representation is shown on Figure \ref{fig::nmf_example}, where the figure below was inspired on the works from \cite{singlechannel}.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.85\textwidth]{"./images/nmf2".png}
    \caption{A visual example of an NMF result, with two sources being separated.}
    \label{fig::nmf_example}
\end{center}
\end{figure}


The algorithms for NMF were further improved by \cite{sparsenmf}, where the authors created a Sparse NMF. Their work was inspired by the basic NMF problems that, when one had overcomplete dictionaries, there was no well defined solution such that you would obtain sparse solutions. Thus, they wanted to make sure to obtain sparse solutions, especially on the coding matrix. To tune the sparsity measurement, the authors simply applied a L1 regularization based on the coding matrix coefficients on the cost functional of the norm, and called it a Sparse NMF. In other words, the cost (or energy) functional that originally was $\mathcal{E} = \| Y - \bar{\bm{D}} \bm{H} \|_F^2$ became

$$\mathcal{E} = \| \bm{Y} - \bar{\bm{D}} \bm{H} \|_F^2 + \underbrace{\lambda \sum_{i,j} H_{ij}}_{L1\text{ regularization}} \quad s.t. \quad \bm{D},\bm{H} \geq 0$$

where $\bar{\bm{D}}$ still follows the notation of \cite{singlechannel}, representing a column-wise normalized dictionary matrix, and $\lambda$ is the parameter to control the degree of sparsity. To solve this optimization problem, if one was to set $\bm{R} = \bm{DH}$, then one would obtain the following algorithm. Note, for this one case, that the dot sign $\cdot$ and and division operator $\frac{a}{b}$ represent point-wise multiplication and division

\begin{codebox}
Sparse NMF Algorithm for Frobenius Norm L1 Regularized Minimization
\begin{align*}
    &1. \text{Initialize }\bm{D,H} \geq 0 \\
    &2. \bm{H}_{ij} \leftarrow \bm{H}_{ij} \cdot \frac{ \bm{Y}_i^T \bar{\bm{D}}_j }{  \bm{R}_i^T  \bar{\bm{D}}_j + \lambda } \quad \quad \quad \quad \quad \quad \quad \quad \in \mathbb{R}^1 \cdot \frac{\mathbb{R}^{1\times M} \mathbb{R}^{M \times 1}}{\mathbb{R}^{1\times M} \mathbb{R}^{M \times 1} + \mathbb{R}^{1}} \\
    &3. \bm{D}_{ij} \leftarrow \bm{D}_{ij} \cdot \frac{ \sum_i \bm{H}_{ij} [ \bm{Y}_i + (\bm{R}_i^T \bar{\bm{D}}_j) \bar{\bm{D}}_j] }{ \sum_i \bm{H}_ij [\bm{R}_i + (\bm{Y}_i \bar{\bm{D}}_j) \bar{\bm{D}}_j ]} \quad \quad \quad \quad \in \mathbb{R}^{M\times 1} \cdot \frac{  \sum_i \mathbb{R}^{1} + (\mathbb{R}^{1\times M} \mathbb{R}^{M\times 1})\mathbb{R}^{M\times 1}  }{ \sum_i \mathbb{R}^{1} [\mathbb{R}^{M \times 1} + (\mathbb{R}^{1\times M} \mathbb{R}^{M\times 1}) \mathbb{R}^{M\times 1}]  }\\
    &4. \text{Repeat 2,3 until stability}
\end{align*}
\end{codebox}

All of the algorithms shown above are the steps necessary to train the algorithm, where $\bm{Y}$ contains all of the training data. In order to actually perform a speech reconstruction, one should simply perform the algorithms above, but to keep the dictionary matrix $\bm{D}$ fixed and update only the code matrix $\bm{H}$. The speech is then separated by conputing the reconstruction of parts of the sparse decomposition. In other words:
\begin{enumerate}
    \item Apply NMF or Sparse NMF to learn the dictionaries of individual speakers
    \item To separate the mixtures, keep $\bm{D}$ fixed, and only update $\bm{H}$
    \item Reconstruct the signal by using the desired parts of the decomposition
\end{enumerate}


\subsubsection{Aproaches to Learn Dictionaries}
As shown in \cite{singlechannel}, there are multiple approaches to perform BSS with NMF or SNMF: unsupervised approach and segmenting the training data and solving multiple NMF problems

In the unsupervised approach, one would compute the SNMF over a very large dataset matrix $\bm{Y}$ of a single speaker to obtain a single dictionary. This, however, may take a lot of computation time. The second approach, used in \cite{singlechannel} involved segmenting the training data according to phoneme labels obtained by a speech recognition software with hidden-Markov models (HMM) for a single speaker, and creating a sparse dictionary for each phoneme, and finally , a final dictionary would be constructed by the concatenation of individual phoneme dictionaries. In order to create the dictionary of each phoneme, each type of phoneme would be concatenated together, to create that phoneme's dictionary, and then, the phonemes dictionaries would be concatenated together to create a dictionary that is representative of the speaker. Figure \ref{fig::speaker_dict} shows an illustration on how to create the dictionary.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.45\textwidth]{"./images/speaker_dict".png}
    \caption{An illustration on how to create a speaker's dictionary $\bm{D}_i$}
    \label{fig::speaker_dict}
\end{center}
\end{figure}

\subsubsection{NMF for Speech Enhancement}
The works from \cite{nmf_denoising} have shown that it is also possible to perform speech enhancement on a speech signal via non-negative matrix factorization. An illustration on Figure \ref{fig::nmf_denoising}, inspired by the images on \cite{nmf_denoising}, shows the procedure. It is possible to generate the dictionary matrix $\bm{D}$ and the coding matrix $\bm{H}$ from a noisy signal, however one would not know which of the subdictionaries $\bm{D}_i$ or the code submatrices $\bm{H}_i$ represent the ones associated with the speech and with the noise. Through spectral clustering (for technical details, see \ref{app:spectralclustering}), it is possible to group the subparts of the dictionary matrix and the code matrix so that it is differentiated between speech and noise. Given that the reconstruction process is based on linear algebra rules, it is then possible to simply multiply the dictionary matrix related to speech $\bm{D}_{speech}$ and the code matrix related to speech $\bm{H}_{speech}$ to obtain the cleaned speech.

Of course this solution is not guaranteed to remove every single noise source due to the possible stochasticity that is propagated to the eigenvectors of the Laplacian, and the fact that the solution is not unique, which is based on the initialization of the centroids of the k-means algorithm, but it could be a good fundamental method to implement.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.4\textwidth]{"./images/nmf_denoising".png}
    \caption{An illustration on how to use non-negative matrix factorization to denoise a signal, where $\bm{Y}$ represents the noisy signal, $\bm{D}$ represents the dictionary matrix obtained through NMF, and $\bm{H}$ represents the code matrix, also attained through the NMF. }
    \label{fig::nmf_denoising}
\end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Speech Recognition}
\subsection{Hidden-Markov Models and Gaussian Mixture Models} \label{sec:hmm}
A well documented work that I've found on Gaussian mixture models was \cite{GMM1}, though works on the Expectation-Maximization (EM) algorithm have been shown as early as in \cite{dudaHart1973}. Such models are often used for unsupervised clustering classification problems, where one assumes the probability distribution of different classes to follow a Gaussian. As mentioned before, it is critical that the data has compactness in order to make the assumption that the data distributions follow Gaussians, and if not, spectral clustering (see \ref{app:spectralclustering}) may be ussed for clustering problems. Nonetheless, for speech, depending on the features that one may be using, it is very safe to assume that they follow Gaussian distributions, which has led a lot of work in speech to be dependent on derivatives of Gaussian mixture models. For more detailed information on GMMs, please see \ref{app:gmm}

One of the most important works for speech are the hidden Markov models (HMM), introduced by \cite{rabiner_juang_hmm} (which happens to be yet one of the best tutorials on HMMs), and shown to work greatly on speech \cite{rabiner_hmm_speech}. The Viterbi algorithm, which was first proposed in \cite{viterbi_original}, and later formalized in \cite{viterbi_algorithm}, is a great way to the solve HMM problems, via dynamic programming. Many consider HMMs to be dynamic forms of GMMs, where a single point in time of a HMMs is essentially a GMM (assuming a Gaussian distribution between the classifications), but this fails if the prior probabilities are not Gaussian. Although, most of the people in the machine learning field use GMMs for clustering classification, it was primarily created in order to create models that, when brought together, created a better representation of an unknown distribution, in the same way that boosting is used to combine different ML methods that complement each other to create a better regressor or classifier. Nonetheless, it became very common to use HMMs for speech recognition. For instance, the work from \cite{singlechannel} could not be completed if the authors did not have an HMM module to output phonemes to segment their signals in order to create dictionaries for SNMF.


\begin{figure}[h]
\begin{center}
    \includegraphics[width=0.28\textwidth]{"./images/HMM_simple".png}
    \includegraphics[width=0.53\textwidth]{"./images/HMM_unrolled".png}
    \caption{ A simple HMM with the states being the phonemes (shown as ARPABET). The image on the left shows the HMM in its compact form, and the image on the right shows the HMM unrolled over time.}
    \label{fig::hmm_simple_unrolled}
\end{center}
\end{figure}

As an example, Figure \ref{fig::hmm_simple_unrolled} shows an example of a HMM, where the states are the phonemes of the word "shoot". As it can be seen, there are multiple entry points to the model, where they can start at SH, UW or T, and as time progresses, which is demonstrated as the repetition of the three states to the right, it is possible that the SH will progress to SH, or any of the other phonemes, and this propagates throughout the entire signal. A set of features is computed over windows of a signal, and a set of features are extracted at each window. The Baum-Welch algorithm is then used to compute the statistics (i.e. "train") for the nodes and to compute the transition probabilities. Finally, when testing, the windows are classified over which states does it most likely belong to, such as with a GMM. The Viterbi-Trellis algorithm is then used to compute the optimal path over the trellis, thus defining all of the states over time. Because the algorithm is computed over a successive multiplication of the probabilities and transition probabilities, it is very likely that the numbers quickly decrease. Therefore, it is optimal to utilize log computations, especially for computers to be able to maintain float accuracies. Once done so, an optimal path is computed, which is depicted on Figure \ref{fig::hmm_simple_unrolled_soln} as a black line.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=0.7\textwidth]{"./images/HMM_unrolled_soln".png}
    \caption{The theoretical Viterbi algorithm solution of a signal.}
    \label{fig::hmm_simple_unrolled_soln}
\end{center}
\end{figure}

HMMs are often formulated where states have substates. Building on the example above, the phonemes could be broken down to substates, which is shown on Figure \ref{fig::hmm_unrolled_ext}. On the illustration, dashed lines show the intra-state transition within each phonemes, and the solid lines represent the transition from one phoneme to another. Notice how, in a phoneme $X$, $X_1$ can only transition to itself or to a $X_2$. Likewise, $X_2$ can only transition to itself or to $X_3$, and $X_3$ can only transition to itself or to a new initial phoneme state $Y_1$.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=0.85\textwidth]{"./images/HMM_unrolled_ext".png}
    \caption{An extended version of the HMM for three phonemes, in which each phoneme contains three substates. }
    \label{fig::hmm_unrolled_ext}
\end{center}
\end{figure}

Because of the powerful formulation which allows classifications to continuously repeat themselves over the analysis of an entire frame, HMM are one the more powerful techniques for time-series analysis or dynamic datastreams, while also having the ability to perform encoding over a signal.

\subsection{Deep Learning Architectures}
Another methodology that some consider it to be "state-of-the-art" is with deep learning, by using recurrent neural networks, which were previously described in \ref{sec:rnn}, there have been promising results using CNNs \cite{ASR_CNN_end2end}, as well as RNNs \cite{ASR_RNN} \cite{ASR_RNN_end2end}, and LSTMs \cite{ASR_LSTM}. The logic behind utilizing CNNs is the fact that a neuron would be a windowing function and classifier, which captures the content/interpretation of a signal as it slides over time, and thus it activates its successive neurons. However, because the authors provide no incentive of understanding the nature of their experiments, nor do they even include a discussion section over their work, and when they do, they simply state already known concepts in the field or regurgitate the aspects of their architecture, concluding the paper by stating that their future works will be to test on a larger dataset or try different architectures, I will move on with our discussion to more fruitful topics which contain explicit discussions with actual scientific approaches to solving problems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Speaker Verification and Speaker Diarization}
When using an ASR system in a medical setting, privacy and accuracy is paramount. In other words, it is extremely important that medical records do not get leaked for obvious reasons, and to make sure that not anyone is logging information into a patient's medical records is also of extreme importance. The concept of a speaker verification system becomes an attractive mechanism for an ASR, such that it ensures that important information is coming specifically from the physician, and not anyone else.

It is important to state that, in contrary to popular belief, speaker verification is not the same as speaker diarization (sometimes spelled as \textit{diarisation}). Speaker verification is a methodology used to determine whether the speaker is the target speaker or not, and this technology was made for security purposes, which in the end of the pipeline, it terminates with a yes/no answer. Speaker diarization, on the other hand, is used to answer the question: "Who spoke when?" It is, however, often times simple to tie these two subfields together, since computation powers have been becoming very powerful, and many times, the adaptation occurs at the last step switching boolean answers to a multiclass boolean problem or vice-versa, therefore, this section presents technologies for both fields together, since what is really important is the meaty part of the methods.

One technique that has been considered state-of-the-art in the past few years is the i-vector methodology. In order to understand i-vectors, it is important to first understand Joint Factor Analysis (JFA), since i-vectors were based on the concepts from JFA. Prior to going into JFA, it is important to understand the difference between two types of session variations:
\begin{itemize}
    \item \textbf{Inter-Speaker Variation}: This is a variation originated from two utterances from different speakers.
    \item \textbf{Inter-Session Variation}: This is a variation originated from utterances from the same speaker. This may be due to:
    \begin{itemize}
        \item \textit{Channel effects}: when utterances are recorded from different channels (e.g. microphones, environment)
        \item \textit{Intra-Speaker Variation}: when utterances vary due to the speaker's health or emotional state
    \end{itemize}
\end{itemize}

\subsection{Gaussian Mixture Model-Universal Background Model and Supervectors}

\begin{figure}[h]
\begin{center}
    \includegraphics[width=0.85\textwidth]{"./images/UBM_speaker".png}
    \caption{An illustration of a speaker model and the adapted speaker model. }
    \label{fig::ubm_speaker_model}
\end{center}
\end{figure}

The Gaussian Mixture Model - Universal Background Model (GMM-UBM) approach was first introduced in \cite{first_UBM}, which is a different approach to doing speaker verification. As mentioned before, GMMs are useful for classifications, but they are also good for creating complex models that cannot be easily modeled after a closed form solution. The GMM-UBM model takes advantage of that.

The UBM is built on a large dataset containing the background (or world) model. In other words, it contains data about the not-the-target, and this largely comprises the other speakers. A very large GMM is trained on it, and then the final UBM is obtained. Then the speaker model is created with a GMM such that it is adapted based on the UBM model. The illustration on Figure \ref{fig::ubm_speaker_model} shows the UBM in blue and the adapted speaker model in yellow (I'm colorblind, so I'm uncertain if this is correct).

\subsubsection*{Maximum A Posteriori Adaptation of the UBM}
Let $X$ represent the acoustic feature vectors by a speaker $s$, the 



One great advantage of using the UBM model is that it has a very nice performance, even if the speaker-dependent data is small; however, the disadvantage is that it requires a gender-balanced large speakers set in order to train it efficiently \cite{advantageUBM}.

\subsection{Joint Factor Analysis}
\subsection{$i$-vectors}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Features from Speech}
