@article{einstein,
  author =       "Albert Einstein",
  title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
                 [{On} the electrodynamics of moving bodies]",
  journal =      "Annalen der Physik",
  volume =       "322",
  number =       "10",
  pages =        "891--921",
  year =         "1905",
  DOI =          "http://dx.doi.org/10.1002/andp.19053221004"
}


@misc{knuthwebsite,
    author    = "Donald Knuth",
    title     = "Knuth: Computers and Typesetting",
    url       = "http://www-cs-faculty.stanford.edu/\~{}uno/abcde.html"
}


@article{blackbox,
    Author = {Ravid Shwartz-Ziv and Naftali Tishby},
    Title = {Opening the Black Box of Deep Neural Networks via Information},
    Year = {2017},
    Eprint = {1703.00810},
    Eprinttype = {arXiv},
}

@article{deeprelaxation,
    author="Chaudhari, Pratik
    and Oberman, Adam
    and Osher, Stanley
    and Soatto, Stefano
    and Carlier, Guillaume",
    title="Deep Relaxation: Partial Differential Equations for Optimizing Deep Neural Networks",
    journal="Research in the Mathematical Sciences",
    year="2018",
    month="Jun",
    day="28",
    volume="5",
    number="3",
    pages="30",
    abstract="Entropy-SGD is a first-order optimization method which has been used successfully to train deep neural networks. This algorithm, which was motivated by statistical physics, is now interpreted as gradient descent on a modified loss function. The modified, or relaxed, loss function is the solution of a viscous Hamilton--Jacobi partial differential equation (PDE). Experimental results on modern, high-dimensional neural networks demonstrate that the algorithm converges faster than the benchmark stochastic gradient descent (SGD). Well-established PDE regularity results allow us to analyze the geometry of the relaxed energy landscape, confirming empirical evidence. Stochastic homogenization theory allows us to better understand the convergence of the algorithm. A stochastic control interpretation is used to prove that a modified algorithm converges faster than SGD in expectation.",
    issn="2197-9847",
    doi="10.1007/s40687-018-0148-y",
    url="https://doi.org/10.1007/s40687-018-0148-y"
}



@article{meanfield,
    author="E, Weinan
    and Han, Jiequn
    and Li, Qianxiao",
    title="A Mean-Field Optimal Control Formulation of Deep Learning",
    journal="Research in the Mathematical Sciences",
    year="2018",
    month="Dec",
    day="13",
    volume="6",
    number="1",
    pages="10",
    abstract="Recent work linking deep neural networks and dynamical systems opened up new avenues to analyze deep learning. In particular, it is observed that new insights can be obtained by recasting deep learning as an optimal control problem on difference or differential equations. However, the mathematical aspects of such a formulation have not been systematically explored. This paper introduces the mathematical formulation of the population risk minimization problem in deep learning as a mean-field optimal control problem. Mirroring the development of classical optimal control, we state and prove optimality conditions of both the Hamilton--Jacobi--Bellman type and the Pontryagin type. These mean-field results reflect the probabilistic nature of the learning problem. In addition, by appealing to the mean-field Pontryagin's maximum principle, we establish some quantitative relationships between population and empirical learning problems. This serves to establish a mathematical foundation for investigating the algorithmic and theoretical connections between optimal control and deep learning.",
    issn="2197-9847",
    doi="10.1007/s40687-018-0172-y",
    url="https://doi.org/10.1007/s40687-018-0172-y"
}
@article{voicerecog_history,
    title = "The Machines that Learned to Listen",
    author = "Katia Moskvitch",
    year = "2017",
    month = "February",
    journal = "BBC Future",
    howpublished = {\url{http://www.bbc.com/future/story/20170214-the-machines-that-learned-to-listen}}
    }

@article{asr_at_home,
    title = {I live with {A}lexa, {G}oogle {A}ssistant and {S}iri. Here's which one you should pick.},
    author = {Geoffrey Fowler},
    year = "2018",
    month = "November",
    journal = "The Washington Post: The Switch Review",
    howpublished = {\url{https://www.washingtonpost.com/technology/2018/11/21/i-live-with-alexa-google-assistant-siri-heres-which-you-should-pick/?noredirect=on&utm_term=.6367751690f2}}
    }
@article{rnn1,
    title={Attractor Dynamics and Parallelism in a Connectionist Sequential Machine},
    author = {Michael I. Jordan},
    year = {1986},
    journal = {Cognitive Science Conference},
    pages = {531--546}
}
@article{rnn2,
    title = {Learning State Space Trajectories in Recurrent Neural Networks},
    author = {Barak, A. Pearlmutter},
    year = {1988},
    journal = {Neural Computation},
    volume = {1},
    pages = {263--269}
}

@article{rnn3,
    title = {Finite-state automata and simple recurrent networks},
    author = {Axel Cleeremans and David Servan-Schreiber and James L. McClelland},
    year = {1989},
    journal = {Neural Computation},
    volume = {1},
    number = {3},
    pages = {372--381}
}

@article{lstm1,
    title = {Long Short-Term Memory},
    author = {Sepp Hochreiter and Jürgen Schmidhuber},
    journal = {Neural Computation},
    year = {1997},
    volume = 9,
    number = 8,
    pages = {1735--1780},
    DOI = {https://doi.org/10.1162/neco.1997.9.8.1735}
}

@article{gru1,
  title={Learning to Forget: Continual Prediction with LSTM},
  author={Felix A. Gers and J{\"u}rgen Schmidhuber and Fred A. Cummins},
  journal={Neural Computation},
  year={2000},
  volume={12},
  pages={2451-2471}
}
}

@INPROCEEDINGS{spectralclustering,
    author = {Andrew Y. Ng and Michael I. Jordan and Yair Weiss},
    title = {On Spectral Clustering: Analysis and an algorithm},
    booktitle = {ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS },
    year = {2001},
    pages = {849--856},
    publisher = {MIT Press}
}
@INPROCEEDINGS{sparsenmf,
    author={J. {Eggert} and E. {Korner}},
    booktitle={2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)},
    title={Sparse Coding and {NMF}},
    year={2004},
    volume={4},
    number={},
    pages={2529-2533 vol.4},
    keywords={matrix decomposition;encoding;sparse coding;NMF;nonnegative matrix factorization;parameter-free method;multivariate data decomposition;sparse coding paradigms;multiplicative algorithm;visual processing map;Europe;Matrix decomposition;Sparse matrices;Time factors;Additives;Principal component analysis;Vector quantization;Visual system;Encoding;Cost function},
    doi={10.1109/IJCNN.2004.1381036},
    ISSN={1098-7576},
    month={July}
}
@inproceedings{singlechannel,
  title={Single-Channel Speech Separation using Sparse Non-Negative Matrix Factorization},
  author={Mikkel N. Schmidt and Rasmus Kongsgaard Olsson},
  booktitle={INTERSPEECH},
  year={2006}
}

@online{rrnoise,
    author = "Jean-Marc and Xiph.Org and Mozilla",
    title = {{RRN}oise},
    year = {2017},
    urldate = {2019-04-24},
    url = {https://people.xiph.org/~jm/demo/rnnoise/}
}

@article{pmf,
    author = {Paatero, Pentti and Tapper, Unto},
    title = {Positive matrix {F}actorization: A Non-Negative Factor Model with Optimal Utilization of {e}rror estimates of data values},
    journal = {Environmetrics},
    volume = {5},
    number = {2},
    pages = {111-126},
    keywords = {Factor analysis, Principal component analysis, Weighted least squares, Alternating regression, Error estimates, Scaling, Repetitive measurements},
    doi = {10.1002/env.3170050203},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.3170050203},
    year = {1994}
}

@article{pmf_chem,
    title = "Source identification of bulk wet deposition in Finland by positive matrix factorization",
    journal = "Atmospheric Environment",
    volume = "29",
    number = "14",
    pages = "1705 - 1718",
    year = "1995",
    issn = "1352-2310",
    doi = "https://doi.org/10.1016/1352-2310(94)00367-T",
    url = "http://www.sciencedirect.com/science/article/pii/135223109400367T",
    author = "Pia Anttila and Pentti Paatero and Unto Tapper and Olli Järvinen",
}

@Article{nmf1,
    author={Lee, Daniel D.
    and Seung, H. Sebastian},
    title={Learning the parts of objects by non-negative matrix factorization},
    journal={Nature},
    year={1999},
    volume={401},
    number={6755},
    pages={788-791},
    issn={1476-4687},
    doi={10.1038/44565},
    url={https://doi.org/10.1038/44565}
}
@inproceedings{nmf2,
    author = {Lee, Daniel D. and Seung, H. Sebastian},
    title = {Algorithms for Non-negative Matrix Factorization},
    booktitle = {Proceedings of the 13th International Conference on Neural Information Processing Systems},
    series = {NIPS'00},
    year = {2000},
    location = {Denver, CO},
    pages = {535--541},
    numpages = {7},
    url = {http://dl.acm.org/citation.cfm?id=3008751.3008829},
    acmid = {3008829},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
}

@article{eigenfaces,
    journal = "Journal of Cognitive Neuroscience",
    volume = {3},
    number =1,
    title = {Eigenfaces for Recognition},
    author = "Matthew Turk and Alex Pentland",
    year = 1991,
    publisher = {MIT Press},
    pages = {71-86}
}

@article{nmf_denoising,
    title = {Matrix Factorization for Speech Enhancement},
    author = {Peter Li and Yijun Xiao}
}

@TECHREPORT{JFA1,
    author = {Patrick Kenny},
    title = {Joint factor analysis of speaker and session variability: Theory and algorithms},
    institution = {},
    year = {2005}
}
@ARTICLE{GMM1,
    author={Mário A. T. Figueiredo and Anil K. Jain},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
    title={Unsupervised Learning of Finite Mixture Models},
    year={2002},
    volume={24},
    number={3},
    pages={381-396},
    keywords={convergence;statistical analysis;unsupervised learning;unsupervised learning;finite mixture models;multivariate data;convergence;parametric mixture model;Gaussian mixtures;Unsupervised learning},
    doi={10.1109/34.990138},
    ISSN={0162-8828},
    month={March}
}

@book{dudaHart1973,
  added-at = {2011-09-14T15:29:26.000+0200},
  address = {New Yotk},
  author = {Duda, R. O. and Hart, P. E.},
  biburl = {https://www.bibsonomy.org/bibtex/2098812b16c7081c0db102c1a37615dd3/jil},
  interhash = {b67a0d8f706c8cec908d83df62d3cf2f},
  intrahash = {098812b16c7081c0db102c1a37615dd3},
  keywords = {book classic k k-means kmeans means},
  publisher = {John Willey \& Sons},
  timestamp = {2013-11-23T20:11:51.000+0100},
  title = {Pattern Classification and Scene Analysis},
  year = 1973
}

@article{rabiner_juang_hmm,
    title={An Introduction to Hidden Markov Models},
    author={Lawrence R. Rabiner and Biing-Hwang Juang},
    journal={IEEE ASSP Magazine},
    year={1986},
    volume={3},
    pages={4-16}
}

@ARTICLE{rabiner_hmm_speech,
    author={Lawrence R. Rabiner},
    journal={Proceedings of the IEEE},
    title={A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition},
    year={1989},
    volume={77},
    number={2},
    pages={257-286},
    keywords={Markov processes;speech recognition;ergodic models;hidden Markov models;speech recognition;discrete Markov chains;hidden states;probabilistic function;coin-tossing;balls-in-urns system;left-right models;Tutorial;Hidden Markov models;Speech recognition},
    doi={10.1109/5.18626},
    ISSN={0018-9219},
    month={Feb}
}

@article{viterbi_original,
    author="VITERBI, A. J.",
    title="Error bounds for convolutional codes and an asymptotically optimal decoding algorithm",
    journal="IEEE Transactions on Information Theory",
    ISSN="",
    publisher="",
    year="1967",
    month="",
    volume="13",
    number="",
    pages="260-269",
    URL="https://ci.nii.ac.jp/naid/10022405791/en/",
    DOI=""
}
@ARTICLE{viterbi_algorithm,
    author={G. D. {Forney}},
    journal={Proceedings of the IEEE},
    title={The viterbi algorithm},
    year={1973},
    volume={61},
    number={3},
    pages={268-278},
    keywords={Viterbi algorithm;Markov processes;State estimation;Recursive estimation;Digital communication;Algorithm design and analysis;Decoding;Convolutional codes;Stochastic processes},
    doi={10.1109/PROC.1973.9030},
    ISSN={0018-9219},
    month={March}
}

@article{ASR_RNN,
    author    = {Alex Graves and
               Abdel{-}rahman Mohamed and
               Geoffrey E. Hinton},
    title     = {Speech Recognition with Deep Recurrent Neural Networks},
    journal   = {CoRR},
    volume    = {abs/1303.5778},
    year      = {2013},
    url       = {http://arxiv.org/abs/1303.5778},
    archivePrefix = {arXiv},
    eprint    = {1303.5778},
    timestamp = {Mon, 13 Aug 2018 16:48:55 +0200},
    biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1303-5778},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ASR_RNN_end2end,
    author = {Graves, Alex and Jaitly, Navdeep},
    title = {Towards End-to-end Speech Recognition with Recurrent Neural Networks},
    booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
    series = {ICML'14},
    year = {2014},
    location = {Beijing, China},
    pages = {II-1764--II-1772},
    url = {http://dl.acm.org/citation.cfm?id=3044805.3045089},
    acmid = {3045089},
    publisher = {JMLR.org},
}
@INPROCEEDINGS{ASR_CNN_end2end,
     author = {Zhang, Ying and Pezeshki, Mohammad and Brakel, Philemon and Zhang, Saizheng and Laurent, C{\'{e}}sar and Bengio, Yoshua and Courville, Aaron},
      title = {Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks},
  booktitle = {Interspeech 2016},
       year = {2016},
      pages = {410--414},
        url = {http://dx.doi.org/10.21437/Interspeech.2016-1446},
        doi = {10.21437/Interspeech.2016-1446}
}

@article{ASR_LSTM,
    author    = {Kyu J. Han and
               Akshay Chandrashekaran and
               Jungsuk Kim and
               Ian R. Lane},
    title     = {The {CAPIO} 2017 Conversational Speech Recognition System},
    journal   = {CoRR},
    volume    = {abs/1801.00059},
    year      = {2018},
    url       = {http://arxiv.org/abs/1801.00059},
    archivePrefix = {arXiv},
    eprint    = {1801.00059},
    timestamp = {Mon, 13 Aug 2018 16:49:10 +0200},
    biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-00059},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{advantageUBM,
    title = {Speaker Verification System Using Gaussian Mixture Model and UBM},
    journal = {International Journal of Digital Application and Contemporary Research},
    year = {2012},
    month = {October},
    volume = {1},
    number = 3,
    author = {Mamta Saraswat Tiwari and Piyush Lotia}
}

@article{first_UBM,
    title = "Speaker Verification Using Adapted Gaussian Mixture Models",
    journal = "Digital Signal Processing",
    volume = "10",
    number = "1",
    pages = "19 - 41",
    year = "2000",
    issn = "1051-2004",
    doi = "https://doi.org/10.1006/dspr.1999.0361",
    url = "http://www.sciencedirect.com/science/article/pii/S1051200499903615",
    author = "Douglas A. Reynolds and Thomas F. Quatieri and Robert B. Dunn",
    keywords = "speaker recognition, Gaussian mixture models, likelihood ratio detector, universal background model, handset normalization, NIST evaluation",
    abstract = "Reynolds, Douglas A., Quatieri, Thomas F., and Dunn, Robert B., Speaker Verification Using Adapted Gaussian Mixture Models, Digital Signal Processing10(2000), 19–41. In this paper we describe the major elements of MIT Lincoln Laboratory's Gaussian mixture model (GMM)-based speaker verification system used successfully in several NIST Speaker Recognition Evaluations (SREs). The system is built around the likelihood ratio test for verification, using simple but effective GMMs for likelihood functions, a universal background model (UBM) for alternative speaker representation, and a form of Bayesian adaptation to derive speaker models from the UBM. The development and use of a handset detector and score normalization to greatly improve verification performance is also described and discussed. Finally, representative performance benchmarks and system behavior experiments on NIST SRE corpora are presented."
}

@inproceedings{supervectors,
    year = {1998},
    title = {Eigenvoices for Speaker Adaptation},
    author = {Roland Kuhn and Patrick Nguyen and Jean-Claude Junqua and L Goldwasser and Nancy Niedzielski and S Fincke and M Contolini},
    booktitle = {{ICSLP} 1998, 5th {I}nternational {C}onference on {S}poken {L}anguage {P}rocessing, 30 {N}ovember-4 {D}ecember 1998, {S}ydney, {A}ustralia},
    address = {{S}ydney, {AUSTRALIA}},
    month = {11},
    url = {http://www.eurecom.fr/publication/198}
}

@ARTICLE{ivec,
    author={N. {Dehak} and P. J. {Kenny} and R. {Dehak} and P. {Dumouchel} and P. {Ouellet}},
    journal={IEEE Transactions on Audio, Speech, and Language Processing},
    title={Front-End Factor Analysis for Speaker Verification},
    year={2011},
    volume={19},
    number={4},
    pages={788-798},
    keywords={speaker recognition;support vector machines;front end factor analysis;speaker representation;low dimensional speaker;channel dependent space;variability space;support vector machine;cosine kernel;similarity estimation;decision score;channel compensation technique;within class covariance normalization;linear discriminate analysis;nuisance attribute projection;speaker recognition evaluation;speaker verification;Support vector machines;Kernel;Testing;Linear discriminant analysis;NIST;Speaker recognition;Permission;Natural languages;Speech analysis;Context modeling;Cosine distance scoring;joint factor analysis (JFA);support vector machines (SVMs);total variability space},
    doi={10.1109/TASL.2010.2064307},
    ISSN={1558-7916},
    month={May}
}
@inproceedings{ivec_nativelang,
  title={Native Language Detection Using the I-Vector Framework},
  author={Mohammed Senoussaoui and Patrick Cardinal and Najim Dehak and Alessandro L. Koerich},
  booktitle={INTERSPEECH},
  year={2016}
}
@article{thermo,
    title = {Stochastic Thermodynamics of Learning},
    author = {Goldt, Sebastian and Seifert, Udo},
    journal = {Phys. Rev. Lett.},
    volume = {118},
    issue = {1},
    pages = {010601},
    numpages = {5},
    year = {2017},
    month = {Jan},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevLett.118.010601},
    url = {https://link.aps.org/doi/10.1103/PhysRevLett.118.010601}
}


@inproceedings{entropySGD,
title = "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys",
author = "P. Chaudhari and Anna Choromanska and S. Soatto and Yann LeCun and C. Baldassi and C. Borgs and J. Chayes and Levent Sagun and R. Zecchina",
year = "2017",
language = "English (US)",
booktitle = "International Conference on Learning Representations (ICLR)",

}

@incollection{elasticSGD,
title = {Deep learning with Elastic Averaging SGD},
author = {Zhang, Sixin and Choromanska, Anna E and LeCun, Yann},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {685--693},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5761-deep-learning-with-elastic-averaging-sgd.pdf}
}

@Article{deid,
    author="Neamatullah, Ishna
    and Douglass, Margaret M.
    and Lehman, Li-wei H.
    and Reisner, Andrew
    and Villarroel, Mauricio
    and Long, William J.
    and Szolovits, Peter
    and Moody, George B.
    and Mark, Roger G.
    and Clifford, Gari D.",
    title="Automated de-identification of free-text medical records",
    journal="BMC Medical Informatics and Decision Making",
    year="2008",
    month="Jul",
    day="24",
    volume="8",
    number="1",
    pages="32",
    abstract="Text-based patient medical records are a vital resource in medical research. In order to preserve patient confidentiality, however, the U.S. Health Insurance Portability and Accountability Act (HIPAA) requires that protected health information (PHI) be removed from medical records before they can be disseminated. Manual de-identification of large medical record databases is prohibitively expensive, time-consuming and prone to error, necessitating automatic methods for large-scale, automated de-identification.",
    issn="1472-6947",
    doi="10.1186/1472-6947-8-32",
    url="https://doi.org/10.1186/1472-6947-8-32"
}

@ARTICLE{tutorial,
author={J. H. L. {Hansen} and T. {Hasan}},
journal={IEEE Signal Processing Magazine},
title={Speaker Recognition by Machines and Humans: A tutorial review},
year={2015},
volume={32},
number={6},
pages={74-99},
keywords={speaker recognition;speaker recognition;subjective verification;structured listening;audio sample;reference template;automatic algorithms;voice-activity detection;speaker models;standard evaluation data sets;performance metrics;Speaker recognition;Forensics;Speech recognition;Feature extraction;Authentication;Noise measurement;Audio systems},
doi={10.1109/MSP.2015.2462851},
ISSN={1053-5888},
month={Nov},}
